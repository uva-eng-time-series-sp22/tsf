[["index.html", "Preface", " Time Series and Forecasting: A Project-based Approach with R Arthur Small Version of: 2022-03-19 Preface This document is a compilation of class notes for SYS 5581 Time Series and Forecasting, University of Virginia, Spring, 2021. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.8 ## ✓ tidyr 1.2.0 ✓ stringr 1.4.0 ## ✓ readr 2.1.2 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(fpp3) ## ── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ── ## ✓ lubridate 1.8.0 ✓ feasts 0.2.2 ## ✓ tsibble 1.1.1 ✓ fable 0.3.1 ## ✓ tsibbledata 0.4.0 ## ── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ── ## x lubridate::date() masks base::date() ## x dplyr::filter() masks stats::filter() ## x tsibble::intersect() masks base::intersect() ## x tsibble::interval() masks lubridate::interval() ## x dplyr::lag() masks stats::lag() ## x tsibble::setdiff() masks base::setdiff() ## x tsibble::union() masks base::union() "],["preface-1.html", "Preface Readings and references Acknowledgements", " Preface This document contains class notes and other materials related to SYS 5581 Time Series and Forecasting at the University of Virginia. Readings and references Time series FPP3 = Hyndman, R.J., &amp; Athanasopoulos, G. (robjhyndmanForecastingPrinciplesPractice2021?) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3 TFS = [these notes] Statistics with R Data science with R, general R4DS = Wickham, Hadley, and Garrett Grolemund, R for Data Science TSDS = Carrie Wright, Shannon E. Ellis, Stephanie C. Hicks and Roger D. Peng, Tidyverse Skills for Data Science Tibshirani, Ryan, Statistics 36-350 Statistical Computing, Carnegie-Mellon University, Fall 2019 Other references on Zotero A variety of other references to resources on time series and forecasting are gathered in the Zotero library for this course. Acknowledgements These notes are organized using the bookdown package (Xie 2022), which was built on top of R Markdown and knitr (Xie 2015). References "],["course-syllabus.html", "Course Syllabus Course Description", " Course Syllabus title: “SYS 5581 Time Series and Forecasting” author: “Instructor: Arthur Small” date: “University of Virginia Engineering, Spring 2021” Class meetings: MW 09:30-10:45 a.m. online via Zoom Office Hours: MW 11:00 a.m.-12:30 p.m. online via Zoom (subject to change). Sign up in advance for a 45-minute session via the Collab “Sign Up” tool. If you cannot make any scheduled time, please contact the instructor via email to schedule an appointment. Meetings online via Zoom: https://virginia.zoom.us/my/arthursmalliii Web Resources: Collab class site, for basic course information, assignments, office hours sign-up, links to online textbook and other resources. Github class site, for posting and sharing code. Zoom, for class sessions, recordings, and office hours. Course Description The course is designed to introduce graduate students and advanced undergraduates in engineering to time series and forecasting. The course will not include a deep exploration of theory. Rather, the goal is for students by the end to be able to analyze time series data competently, as part of their work designing and working with engineered systems. In addition to learning theory, each student will undertake a semester-long research project. Ideally, this project will relate closely to the student’s own dissertation research, professional practice, or other domain application that interests them. My hope is that these projects could form the basis for subsequent research papers, dissertation chapters, or other professional work products, for interested students. The course will, therefore, be structured primarily as a workshop: the ultimate goal is to help you to create a professionally presented report. Our workflow will, therefore, be subject to revision, according to my judgement of how best to use our time to help you produce a professional report. The course outline is divided into two major sections. First, we will introduce the theory, with examples. In the later part of the semester, we will focus on workshopping your projects in progress. Important: class readings are subject to change, contingent on mitigating circumstances and the progress we make as a class. Students are encouraged to attend lectures and check the course website for updates. Prerequisites Students should have taken at least one rigorous intermediate course in probability and statistics. They should be comfortable with the representation of uncertain information in the form of probability distributions, with conditional probabilities, and with other such foundational concepts. In addition, to carry out the data analyis, the student should have at least ability be able to code, in some general-purpose language. For this course, we will work in R, focusing on specialized packages for working with time series data and generating forecasts. Expectations Each student will make a presentation on their data analysis project. Students will be evaluated based on their performance in these presentations and on their final project, on occasional short quizzes; and on their contributions inside and outside of class towards helping other students. Readings The primary text for the course will be Forecasting: Principles and Practice, 3rd ed. by Rob J. Hyndman and George Athanasopoulos. This resource is available for free online and is linked from the Collab site. The text includes example code in R, and covers several useful R packages related to time series and forecasting. Additional readings including relevant articles will be provided as the course progresses. The choice of readings will depend in part on student interests, as conveyed through their choice of projects. Course Objectives Students will learn the foundations of time series and forecasting. Students will gain the experience of building statistical models of time series, and models for forecasting, and will learn how to evaluate their performance. Students will learn the concepts and practice of reproducible research, in the course of preparing a research paper. Students will gain experience in making presentations and in preparing a polished research article. Grading Policy 10% of your grade will be determined by quizzes designed to test your understanding of the theoretical concepts introduced in class. This quiz will delivered at roughly the mid-point of the semester. It will be open-book and open-notes, outside of class. You will have multiple days to complete it. The quiz will not be designed to be especially challenging: the goal is to give you the opportunity to synthesize your understanding of core concepts, in preparation for developing your data analysis for your research project. 10% of your grade will be determined by your performance in one in-class presentation based on your project. These presentations will be scheduled when you are, in the judgement of the instructor, far enough along to do so. 10% of your grade will be determined by your contributions to assist other students. These contributions can come through class participation, by making useful contributions in online forums (Github), or through other means that add value to the group experience. 70% of your grade will be determined by your performance on your final project. The development of the project will include multiple iterations, each with an associated deliverable: An initial Concept Note. A more developed Project Proposal. A first working draft of your final project paper. A final complete draft of your project paper. Details of these staged intermediate deliverables will be forthcoming. The final product should be a polished professional paper that meets academic standards regarding format, quality, and integrity. Attendance Policy Regular attendance is very much in your pedagogic interest. However, it is up to you whether to attend in person or to view recorded class sessions afterwards. Communications protocols, including emails and office hours I prefer to avoid using email to communicate with students about class matters. For substantive questions about course materials and concepts, please use class time, office hours, or meetings by appointment. Please use email only for brief clarifying questions, or to set up appointments. Academic Dishonesty Policy Don’t cheat. Don’t plagiarize. Don’t present someone else’s work as your own. Disabilities Policy Together with the University of Virginia, I am committed to assuring that all students have the full opportunity to benefit from the course regardless of their disability status. If you have a disability that may require accommodations, please see the instructor early in the semester to work out appropriate arrangements. "],["intro.html", "Chapter 1 Introduction 1.1 What is time series analysis? 1.2 Time series data 1.3 Time series patterns 1.4 Types of problems that are amenable to time series analysis 1.5 Time series and forecasting 1.6 Overview of the course", " Chapter 1 Introduction Readings: FPP3, Ch. 1 1.1 What is time series analysis? 1.2 Time series data 1.3 Time series patterns Examples. 1.4 Types of problems that are amenable to time series analysis 1.5 Time series and forecasting 1.6 Overview of the course "],["project-workflow.html", "Chapter 2 Project workflow Example problem: Estimating the probability of a weather event 2.1 State your question 2.2 Acquire data and background information 2.3 Organize your data 2.4 Perform exploratory analysis of your data 2.5 Write down your model of the data generating process 2.6 If necessary: transform the model and data to make it ready for analysis 2.7 Choose an appropriate technique for estimating model parameters, consistent with your assumptions about your data generating process 2.8 Estimate model parameters 2.9 Confirm that your modeling assumptions are satisfied 2.10 Compute measures of model quality; confirm that your model is good enough for your purpose 2.11 Use your calibrated model to address your original question", " Chapter 2 Project workflow Readings: FPP3, Section 5.1 TSDS, Sections 5.1-5.3 Case study examples: Open Case Studies: Exploring CO2 emissions across time Open Case Studies: Predicting Annual Air Pollution Steps in a time series statistical analysis: State your question Acquire data and background information Organize your data Perform exploratory analysis of your data Write down your model of the data generating process If necessary: transform the model and data to make it ready for analysis Choose an appropriate technique for estimating model parameters, consistent with your assumptions about your data generating process Estimate model parameters Confirm that your modeling assumptions are satisfied; Compute measures of model quality; confirm that your model is good enough for your purpose Use your calibrated model to address your original question. Examples of using your model: Generate a forecast of future events Estimate the probability of a future event \\(\\ldots\\) Example problem: Estimating the probability of a weather event A pub owner in Charlottesville plans to sell beer outside on St. Patrick’s Day, March 17. The pub owner must decide whether to arrange to rent a supplemental refrigeration system for the day. Supplemental refrigeration offers a form of insurance. If temperatures outside on March 17 are high and the pub owner has not arranged supplemental refrigeration, she will be left with warm beer that she will have difficulty selling, leading to financial losses. Conversely, if she pays for supplemental refrigeration when temperatures are low, she will have incurred an unnecessary expense. To decide whether to insure herself against loss, she wishes to estimate the probability distribution of temperatures on March 17. 2.1 State your question 1. What is the probability that the high temperature on March 17 will exceed 23 degrees Celsius? 2.2 Acquire data and background information Get historical temperature data. (Here, will simulate the data.) set.seed(1) # Keeps random data from changing each time the code is run theta &lt;- 18 # True long-run average temp sigma &lt;- 3 # True stnd dev of temp around this mean n &lt;- 40 # n = number of simulated historical data points # We don&#39;t use &#39;T&#39; because in R language &#39;T&#39; = logical &#39;TRUE&#39; y &lt;- rnorm(n,theta,sigma) 2.3 Organize your data Let \\(y_1, \\ldots, y_T\\) denote the high temperature in Charlottesville on March 17 for each of the previous \\(T\\) years. print(y) ## [1] 16.12064 18.55093 15.49311 22.78584 18.98852 15.53859 19.46229 20.21497 ## [9] 19.72734 17.08383 22.53534 19.16953 16.13628 11.35590 21.37479 17.86520 ## [17] 17.95143 20.83151 20.46366 19.78170 20.75693 20.34641 18.22369 12.03194 ## [25] 19.85948 17.83161 17.53261 13.58774 16.56555 19.25382 22.07604 17.69164 ## [33] 19.16301 17.83858 13.86882 16.75502 16.81713 17.82206 21.30008 20.28953 2.4 Perform exploratory analysis of your data hist(y, breaks = 20, main = &quot;Histogram of historical temperatures in degrees C&quot;) boxplot(y) 2.5 Write down your model of the data generating process It is supposed that these data were generated as independent, identically distributed random draws from a normal distribution: for \\(t = 1, \\ldots, T\\), \\[y_t = \\theta + \\varepsilon_t\\] where \\(\\theta\\) denotes the true but unobserved value of the long-run average temperature, and where \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\). 2.5.1 Comments on this statistical model: The risk of model mis-specification This model asserts several substantive assumptions about the data generating process. The process is assumed to be stationary. There is no upward trend over time, no long-term climate change, etc. Temperatures are assumed to be independent from one year to the next. In particular, there is no autocorrelation. Knowing that one year’s temperature was unusually high (say) provides no information about the likelihood that next year’s temperature will also be unusually high. Inter-annual climate cycles (e.g., due to \\(El\\ Ni\\tilde{n}o\\)) are ruled out. Temperature variations around the long-run average are assumed to be identically distributed. This assumption rules out the possibility that variance is, say, greater when temperatures are higher than when they are lower. And others. In general, it is important to formulate a statistical model that accurately reflects the true characteristics of the underlying data generating process. When your statistical model is mis-specified, your probabilistic forecast of future events are likely to stray from the true underlying probabilities. Model mis-specification can then lead to inaccurate estimates of the distribution of losses for each possible action. This error may in turn lead to selection of a sub-optimal action. A particular problem to guard against is the possibility to underestimate the likelihood of extreme events that could cause catastrophic losses. That said, your time on this Earth is limited. Depending on the decision problem and the stakes involved, refining your model to get sharper loss estimates may or may not be worth the bother. A reasonable approach is to start by first writing down a simple forecasting model that appears to capture the essence of the process as you understand it. On the basis of this simple model, generate first-cut probabilistic forecasts of uncertain events. Use these to generate estimated distributions of losses for each possible action in your action set. On that basis, use the specified decision criterion to derive an initial optimal decision rule. Then, go back and check things over more carefully. Review the realism of your statistical model, given your understanding of data generating process. Plot and examine the distribution of your prediction errors. Do your prediction errors appear to follow a pattern that matches what you would expect, given the assumptions you have made? If you find evidence that your prediction model is mis-specified, it may be worth it to go back and refine your model, and see if you generate different results. One very good idea is to perform a sensitivity analysis. How sensitive are your decision recommendations and outcomes to the assumptions you’ve built into your statistical model? If you are not highly confident in your statistical assumptions, and if those assumptions turn out to matter a lot for your recommendations and outcomes, then it could very well be worth the bother to revisit those assumptions, and investigate alternatives. On the other hand, if your decision recommendations are not highly sensitive to your statistical assumptions, then keeping your initial model may be defensible. The point of this work is not to build the best possible prediction system, bullet-proof against any statistical criticism. The point is to help people make good decisions – or at least, decisiions better than they would have made otherwise. Your time and other resources are limited. A good-enough model may be good enough. 2.6 If necessary: transform the model and data to make it ready for analysis Not needed here. Will consider many cases where it is. 2.7 Choose an appropriate technique for estimating model parameters, consistent with your assumptions about your data generating process For this case, ordinary least squares (OLS) estimation is just fine. 2.8 Estimate model parameters theta_hat &lt;- mean(y) # Sample mean epsilon_hat &lt;- y-theta_hat # Model residuals ssr &lt;- sum(epsilon_hat^2) # Sum of squared residuals sigma_hat &lt;- ssr/(n-1) # Estimated standard error print(theta_hat) ## [1] 18.27608 print(sigma_hat) ## [1] 7.075605 2.9 Confirm that your modeling assumptions are satisfied 2.10 Compute measures of model quality; confirm that your model is good enough for your purpose forecast_bias &lt;- -0.5 forecast_std_error &lt;- 1 forecast_errors &lt;- rnorm(n,mean = forecast_bias,sd = forecast_std_error) x &lt;- y + forecast_errors linear_model &lt;- lm(y~x) plot(x,y, xlab = &quot;Forecast temperature&quot;, ylab = &quot;Observed temperature&quot;) abline(linear_model) plot(x,y) 2.11 Use your calibrated model to address your original question sim_y &lt;- rnorm(10000, theta_hat, sigma_hat) hist(sim_y, breaks = 100) "],["assignment-write-a-concept-note.html", "Chapter 3 Assignment: Write a concept note 3.1 Assignment Instructions 3.2 Submission procedure 3.3 Choosing a topic", " Chapter 3 Assignment: Write a concept note Write a concept note for a potential time series analysis project. 3.1 Assignment Instructions In just a few paragraphs (1 page max), describe an application of time series analysis that you might undertake in connection with your own research, your professional practice, or for a class project. 3.2 Submission procedure Prepare your note in R Markdown within R Studio. Use knitr to generate a PDF output. Commit your saved .Rmd and .pdf files to your (local) repo, then push your changes to your repo on the course site on Github. When all is ready, submit your assignment on Collab. But don’t attach your pdf file. Instead, a link to your .pdf file on Github. Generate your output as a PDF rather than as HTML: Github makes is complicated to download and view .html files. 3.3 Choosing a topic Your topic might depend on which type of degree program you are in, and how far along you are in that program. If you are well along in a research-oriented graduate degree program, then you likely have already a fairly clear notion of the research question(s) you are addressing or plan to address, and the methods you will use to approach your question. Ideally, you will have already secured access to a time series data set that you hope to analyze. In this case, please describe the data set, and the type of useful information you hope to learn about it. If applicable, describe how this analysis could inform your research program. If you at the beginning stages of a research-oriented degree program, you likely have some idea of the topics and methods you will use, but may not have much idea of what data sets you will use or what insights you hope to extract from those data. In this case, please think about what kinds of time series data analysis might help advance your work. It is likely that a consultation with your research advisor will be helpful. Write about your research question(s), and how analysis of time series data might help your research program. Ideally, please try to identify a data set that you could use, and describe how you can access these data. If you are not in a research-oriented degree program, think about how a time series analysis could be applied to some aspect of your work, professional practice, or personal or career interests. If you don’t have any ideas at all, and are looking for help finding one, let me know. We will identify together a project idea and data set you can work with, possibly related to energy. But try first to think of one on your own, one that you care about. "],["project-set-up-good-practices.html", "Chapter 4 Project set up: Good practices 4.1 Reproducible workflows 4.2 Setting up a new project 4.3 Folder structure 4.4 Naming things", " Chapter 4 Project set up: Good practices 4.1 Reproducible workflows 4.2 Setting up a new project 4.3 Folder structure Readings: TSDS Section 1.6 4.4 Naming things Readings: Jenny Bryan, naming things slide deck, Reproducible Science Workshop, 2015. Hadley Wickham, The tidyverse style guide. Section 1: “Files” "],["assignment-set-up-your-computing-environment.html", "Chapter 5 Assignment: Set up your computing environment 5.1 The R programming language, and related resources 5.2 Git and Github 5.3 Markdown and R Markdown 5.4 Bibliographic resources: Zotero and Bibtex 5.5 General course web resources", " Chapter 5 Assignment: Set up your computing environment The course relies on computing resources. Please install the software as indicated on your local machine, and familiarize yourself with the associated documentation. Topics: R, R Studio, git, Github, Markdown, R Markdown, Tidyverse and tidyverts packages for R Assignment: Follow instructions in the course Computing setup guide. 5.1 The R programming language, and related resources We will do our coding in R, a programming language especially well-suited to statistical computing. Download and install R, v. 3.0.1+. Note: There is a later version, v. 4.0.2, in development, but you shouldn’t need it. R Studio is an integrated development environment (IDE) for R. It offers a variety of utilities to enhance the experience of coding and generating documents. Download and install R Studio, v. 1.4.1+. Tidyverse is a collection of packages that extend the capabilities of R for doing data science. Install the Tidyverse packages for R: From the Console tab in R Studio (or from R running in a Terminal window), enter: install.packages(&quot;tidyverse&quot;) Alternatively, you may install packages via the Packages tab in R Studio. Optional: To learn how to wrangle and visualize data using the Tidyverse packages, you may find it useful to go through the Tidyverse Fundamentals with R modules on Datacamp. Datacamp also offers a range of other learning modules for developing data science skills in R. Tidyverts is a collection of R packages for time series analysis designed to work well with the Tidyverse packages. Each package in the tidyverts suite needs to be installed individually: From the Console tab in R Studio (or from R running in a Terminal window), enter: install.packages(c(&quot;tsibble&quot;, &quot;tsibbledata&quot;, &quot;feasts&quot;, &quot;fable&quot;)) You don’t need to install the tsibbletalk and fable.prophet packages; we probably won’t use them in this course. 5.2 Git and Github Reference: Happy Git and GitHub for the useR Git is software for version control. Github is a web service that provides remote storage and access to files via git. This setup greatly facilitates collaboration between multiple individuals working on the same code base. First watch this short YouTube video to get an orientation to git and Github: Git and GitHub for an Organized Project (STAT 545 Episode 2-A) from the University of British Columbia. Then install git on your machine and link it to your R Studio instance and your file repository on Github: Follow these instructions to download and install git and to link git with R Studio. A collection of files associated with a single project is in git-speak called a “repository” or “repo”. You should already have a basic repo set up for you on the course site on Github. The next step is to copy (“clone”) this remote repo to your local machine. Clone your course repo on Github to a new R Studio project on your local machine. Navigate to the course website on Github. Select your repo. Click on the green button labeled “Code”. Copy the URL. In the R Studio window, from the pull-down menu in the upper-right corner, select New Project..., Version Control, Git. Paste the URL into the dialog box labeled Repository URL. Optional: Change the name of the project folder, and the location of this folder on your local directory tree. Click on Create Project. The files from your remote repo should be copied to your local machine in a new folder with the name you chose. Optional: Download and install the Github desktop client, or an alternative GUI client. The git operations you need for this course can be managed within R Studio, from the Git tab. Some more advanced operations require using either a Terminal window, or a Git desktop client. As you get going, you will likely want to learn more about how to work with git and Github. Review the documentation for git and this Github Guide. Learn the basics. 5.2.1 Using personal tokens to access Github Github is phasing out the use of passwords for authorizations. ---- Forwarded Message ----- From: GitHub &lt;noreply@github.com&gt; To: Arthur Small &lt;asmall@virginia.edu&gt; Sent: Sunday, February 21, 2021, 6:20:58 AM EST Subject: [GitHub] Deprecation Notice Hi @arthursmalliii, You recently used a password to access the repository at uva-eng-time-series-sp21/coronato-nicholas with git using git/2.30.0. Basic authentication using a password to Git is deprecated and will soon no longer work. Visit https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information around suggested workarounds and removal dates. Thanks, The GitHub Team Instead, you must create a personal access token. See the Github documentation. 5.3 Markdown and R Markdown Markdown is a markup language: a set of formatting instructions for rendering documents. R Markdown is an extension of Markdown that allows for embedding chunks of R code into a Markdown document. In this course, we will write our work in R Markdown within the R Studio environment, then use the knitr package to generate HTML and PDF output files. For a nice introduction to Markdown and R Markdown, watch the short YouTube video Reproducible Reports with R Markdown (STAT 545 Episode 3-A) from the University of British Columbia. As you proceed in creating your documents, you will probably want to access additional resources: From within R Studio, you can access an R Markdown Cheat Sheet via Help/Cheatsheets. Markdown reference: https://www.markdownguide.org/ R Markdown reference: https://rmarkdown.rstudio.com/ 5.4 Bibliographic resources: Zotero and Bibtex [Coming soon…] 5.5 General course web resources Collab class site, for basic course information, assignments, office hours sign-up, links to online textbook and other resources. Github class site, for posting and sharing code. Zoom, for class sessions, recordings, and office hours. "],["finding-a-dataset-for-your-project.html", "Chapter 6 Finding a dataset for your project 6.1 Appropriate data sets for time series analysis 6.2 Data resources", " Chapter 6 Finding a dataset for your project Figure 6.1: New York City 311 calls by time of day, September 8–15, 2010. Source: Wired Magazine Readings: TSDS, Section 5.4 6.1 Appropriate data sets for time series analysis Just because your data set has some dates and times in it, doesn’t mean it is appropriate for a time series analysis. Time series methods are generally appropriate when data are measured at regular intervals, over a fairly long period. 6.2 Data resources The UVA Libraries offer excellent data services, including resources for data discovery and access. If you haven’t settled on your own dataset to analyze for your project, you may find one by browsing their recommended top data sources and licensed data. If you need personal assistance, you are invited to contact UVA’s Data Librarian, Jenn Huck, at data@virginia.edu to schedule an appointment. Some data sources you might check out: The Cross-National Time-Series Data Archive provides more than 200 years of annual data from 1815 onward for over 200 countries. It consists of 196 data variables used by academia, government, finance and media. U.S. Energy Information Administration. Diverse datasets on energy. “Awesome Public Datasets”. Flowing Data data sources. The Stanford Open Policing Project. “On a typical day in the United States, police officers make more than 50,000 traffic stops. Our team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country.” Open U.S. Federal Government Data. Charlottesville, Virginia open data New York City 311 Data. "],["data-acquisition-and-extraction.html", "Chapter 7 Data acquisition and extraction 7.1 Access protocols and permissions 7.2 Accessing databases 7.3 Other comments", " Chapter 7 Data acquisition and extraction Readings: TSDS, Chapter 2 7.1 Access protocols and permissions Reproducible extraction of data from source location: may be complicated by access protocols. access tokens; APIs raw data from github for private repos databases package httr to access data from websites 7.2 Accessing databases esales &lt;- dbGetQuery(db,&#39;SELECT * from eia_elec_sales_va_all_m&#39;) # SQL code to retrieve data from a table in the remote database # str(esales) esales &lt;- as_tibble(esales) # Convert dataframe to a &#39;tibble&#39; for tidyverse work # str(esales) # Reference: https://arrow.apache.org/docs/r/ # if(!(&#39;arrow&#39; %in% installed.packages())) install.packages(&#39;arrow&#39;) library(arrow) write_feather(esales, &quot;esales.feather&quot;) # Close connection -- this is good practice dbDisconnect(db) dbUnloadDriver(db_driver) 7.3 Other comments Make your extraction code “as reproducible as possible”, subject to these access constraints. At minimum, document clearly how you obtained the data, so others could follow your path, even if not via pure code. Keep your raw data in read-only mode. Don’t edit these files. Write code to transform the raw data into form you will use for analysis. Don’t do it manually. "],["assignment-update-your-concept-and-get-your-data.html", "Chapter 8 Assignment: Update your concept and get your data 8.1 Refine your concept note 8.2 Identify a time series data set you want to work with 8.3 Acquire the data from its source location, reproducibly 8.4 Stage your raw data 8.5 Submission procedure 8.6 Other comments", " Chapter 8 Assignment: Update your concept and get your data 8.1 Refine your concept note State clearly the question(s) you will attempt to answer. Make sure this is a clear, answerable question. Don’t do “a study of…”. Make sure this is genuinely a question about a time series process. Imagine a metronome: each time you hear a ‘click’, another row of data values gets added to your table. 8.2 Identify a time series data set you want to work with Ideally, identify a data set that you that you might use as the basis for your class project. You need not necessarily commit at this time to using this data set for your project. However, bear in mind that you will in this assignment be investing time and effort in acquiring, cleaning, and organizing the data set. It is better for you if you invest that effort on the data set you will later analyze. Please do not use a data set that already come packaged with R or that is otherwise already cleaned up. The point of this assignment is to learn how to use tools from the Tidyverse and tidyverts packages when working with data you acquire “in the wild”. 8.3 Acquire the data from its source location, reproducibly You typically will need first to extract your data from its original source (e.g., an Excel file, an API, a database, a cloud hosting service). 8.4 Stage your raw data If your data files are smaller than 25 MB, you may synch them to Github with the rest of your repo. Put them in the raw_data folder. Files larger than 25 MB should not be posted to Github. Instead, post them to a cloud storage service such as Google Drive, One Drive, Box, or Dropbox. Your code should include a reproducible procedure for downloading the data in these files to a local drive. 8.5 Submission procedure Create a new R Markdown document for this work, inside R Studio. Code and document all these steps in a single .Rmd file. To submit your work: knit your .Rmd file to generate a .pdf file. Push both the .Rmd and .pdf files to your Repo on Github, along with any supporting files. Submit your assignment on Collab, enclosing a link to your .pdf file on Github. 8.6 Other comments All steps should be fully reproducible. This means: If I, or anyone, rerun the code in your R Markdown file and reknit the .Rmd file to generate a new PDF output, I should be able to execute all your computational steps and regenerate your PDF essentially exactly. "],["data-preparation-strategy.html", "Chapter 9 Data preparation strategy 9.1 Overview: Extraction, transformation, and loading of data 9.2 Organize your data into a tidy data frame 9.3 Convert your data into a tsibble object 9.4 Data preparation strategy: Design your end-point data table(s) 9.5 Additional resources and next steps", " Chapter 9 Data preparation strategy Topics: Tidy data tsibble objects for storing, manipulating, and visualizing time series data. Frequency of time series: the index parameter. key parameter(s). Applying dplyr verbs to tsibble objects: filter, select, mutate, group_by, summarize Readings: TSDS, Sections 3.1-3.9 FPP3, Section 2.1 Optional: To learn how to wrangle and visualize data using the Tidyverse packages, you may find it useful to go through the Tidyverse Fundamentals with R modules on Datacamp. Datacamp also offers a range of other learning modules for developing data science skills in R. Assignment: Extract and prepare your data. 9.1 Overview: Extraction, transformation, and loading of data Before undertaking any data analysis project, you need to organize your data into a format to make it ready for analysis. Very commonly, the data you wish to work with will not come to you in a nice format that makes it ready to analyze. Often it will be necessary to transform the data, applying a sequence of manipulations to get it into a nice format such as a single table. If you have saved your prepared table to a database or local file, your may finally need to load the data into memory on your working machine as a prelude to commence analysis. These steps together are the extract-transform-load (ETL) stage of a data analysis project. The bad news is that working data scientists generally report that the ETL stage is the most time-consuming part of a data science project. The good news is that the R tidyverse packages offer a number of helpful tools to somewhat ease the pain of ETL work, also known informally as data wrangling.1 The ETL steps needed for a given project will depend on the nature of the data and on how they are originally organized. We can characterize how we want the data to look at the end of the ETL stage. To the extent possible, we want the data to be tidy. 9.2 Organize your data into a tidy data frame Readings: TSDS Sections 1.2-1.3 Hadley Wickham (wickhamTidyData2014?) codifies the concept of tidy data: Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. “Real datasets can, and often do, violate the three precepts of tidy data in almost every way imaginable. While occasionally you do get a dataset that you can start analysing immediately, this is the exception, not the rule.” In real life, the systems people and organizations use to collect, manage, and store data are governed by many priorities: end-user convenience, clarity of presentation, storage costs, processing speed, etc. Making data fit for analysis by data scientists is typically a minor consideration, if it is thought of at all. Expanding on the theme, Wickham identifies five of the most common problems with non-tidy, “messy” datasets: Column headers are values, not variable names. Multiple variables are stored in one column. Variables are stored in both rows and columns. Multiple types of observational units are stored in the same table. A single observational unit is stored in multiple tables. The Tidyverse packages integrate a range of tools to help with transforming the messy data often encountered in the wild into tidy formats more suitable for analysis. See the Tidyverse website for an overview, or this Coursera course for more guidance. Organize your data table(s) so that they ‘match’ your question. 9.3 Convert your data into a tsibble object In this step of the assignment, you will convert your tidy data frame into a tsibble object. Doing so in effect tells R: “These data actually form a time series. One column, which I designate the index, contains time values, in equal intervals.” Taking this step enables the use of specific tools for data visualization, exploratory analysis, and forecasting for time series data. The tsibble package for R provides “a data infrastructure for tidy temporal data”. Review the documentation for instructions. 9.4 Data preparation strategy: Design your end-point data table(s) Starting point: Multiple source files, mess, etc. This is real life as a data scientist. What’s your desired end point? How will you prepare your data to make it ready to analyze? Data preparation is a creative activity. (Your jobs are secure.) 9.4.1 Design your end point first (at least, in your head). Which columns? In which order? Which data types should the different columns have? For tsibble objects: Which column is the index? Must have date+time values, at regular intervals. Which columns contain key values? These are values that don’t change with time. Each value of the key corresponds to a distinct time series. Cannot have duplicate rows that share an index + key value. Remaining columns contain observational data: one row for each time step and key value. What data types should these be? Might choose to drop columns you aren’t going to use, to reduce clutter. 9.4.2 Typical structure for a time series data table Date + time Series Value_1 Value_2 Value_3 2020-02-01 “Virginia” 33.57 29 “friendly” 2020-02-01 “Idaho” 0.22 18 “hostile” … … … … … index key [date] [fctr] [dbl] [int] [fctr] Then wrangle your data to get to your desired end point. Recommended practices: Put index field in the left-most column. Next, put all the key fields. Then finally the data values. Start with the most important ones. 9.5 Additional resources and next steps To learn how to wrangle and visualize data using the Tidyverse packages, you may find it useful to go through the Tidyverse Fundamentals with R modules on Datacamp, or the analogous course on Coursera. Datacamp also offers a range of other learning modules for developing data science skills in R. With your data organized into a tsibble object, you are positioned to do some exploratory data analysis, the topic of the next assignment. If you want to get a head start, check out the functions for data visualization and exploratory analysis of time series in the feasts package. Or work through the examples in Chapter 2 or Chapter 4 of Forecasting: Principles and Practice, 3rd ed. “Wrangling” refers to work with cattle, sheep, and other livestock.↩︎ "],["reading-in-data-from-source-files.html", "Chapter 10 Reading in data from source files 10.1 First, examine text files in a text editor 10.2 Read in CSV files", " Chapter 10 Reading in data from source files 10.1 First, examine text files in a text editor It’s generally a good idea to examine the contents of an unfamiliar text data file visually, in text editor, before starting to work on it. You can open smaller files inside RStudio. For files too big to open inside RStudio, you can use a dedicated text editor like Atom. This code prints out the first ten lines of the text file “PetroStocks.csv”. # Print out first ten rows of file &quot;data/PetroStocks.csv&quot; readLines(&quot;data/PetroStocks.csv&quot;, n=10) %&gt;% paste0(collapse=&quot;\\n&quot;) %&gt;% cat ## Back to Contents,Data 1: Petroleum Stocks,,,,,,,,,,,,,,,,,,, ## Sourcekey,WCRSTUS1,WCESTUS1,WCSSTUS1,WGTSTUS1,WGRSTUS1,WG4ST_NUS_1,WBCSTUS1,W_EPOOXE_SAE_NUS_MBBL,WKJSTUS1,WDISTUS1,WD0ST_NUS_1,WD1ST_NUS_1,WDGSTUS1,WRESTUS1,WPRSTUS1,W_EPPO6_SAE_NUS_MBBL,WUOSTUS1,WTTSTUS1,WTESTUS1, ## Date,Weekly U.S. Ending Stocks of Crude Oil (Thousand Barrels),Weekly U.S. Ending Stocks excluding SPR of Crude Oil (Thousand Barrels),Weekly U.S. Ending Stocks of Crude Oil in SPR (Thousand Barrels),Weekly U.S. Ending Stocks of Total Gasoline (Thousand Barrels),Weekly U.S. Ending Stocks of Reformulated Motor Gasoline (Thousand Barrels),Weekly U.S. Ending Stocks of Conventional Motor Gasoline (Thousand Barrels),Weekly U.S. Ending Stocks of Gasoline Blending Components (Thousand Barrels),Weekly U.S. Ending Stocks of Fuel Ethanol (Thousand Barrels),Weekly U.S. Ending Stocks of Kerosene-Type Jet Fuel (Thousand Barrels),Weekly U.S. Ending Stocks of Distillate Fuel Oil (Thousand Barrels),&quot;Weekly U.S. Ending Stocks of Distillate Fuel Oil, 0 to 15 ppm Sulfur (Thousand Barrels)&quot;,&quot;Weekly U.S. Ending Stocks of Distillate Fuel Oil, Greater than 15 to 500 ppm Sulfur (Thousand Barrels)&quot;,&quot;Weekly U.S. Ending Stocks of Distillate Fuel Oil, Greater Than 500 ppm Sulfur (Thousand Barrels)&quot;,Weekly U.S. Ending Stocks of Residual Fuel Oil (Thousand Barrels),Weekly U.S. Propane and Propylene Ending Stocks Excluding Propylene at Terminal (Thousand Barrels),Weekly U.S. Ending Stocks of Other Oils (Excluding Fuel Ethanol) (Thousand Barrels),Weekly U.S. Ending Stocks of Unfinished Oils (Thousand Barrels),Weekly U.S. Ending Stocks of Crude Oil and Petroleum Products (Thousand Barrels),Weekly U.S. Ending Stocks excluding SPR of Crude Oil and Petroleum Products (Thousand Barrels), ## &quot;Aug 20, 1982&quot;,609219,338764,270455,,,,,,33523,149415,,,,51168,,,119293,,, ## &quot;Aug 27, 1982&quot;,608741,336138,272603,,,,,,33897,154589,,,,48544,,,119863,,, ## &quot;Sep 24, 1982&quot;,612419,335586,276833,,,,,,34949,158684,,,,57813,,,117686,,, ## &quot;Oct 01, 1982&quot;,612419,334786,277633,,,,,,33919,154461,,,,60828,,,118759,,, ## &quot;Oct 08, 1982&quot;,613985,335260,278725,,,,,,32347,158242,,,,60381,,,119017,,, ## &quot;Oct 15, 1982&quot;,607781,326979,280802,,,,,,33382,161578,,,,60929,,,117603,,, ## &quot;Oct 22, 1982&quot;,617763,334370,283393,,,,,,34280,162934,,,,61688,,,115236,,, What do you see? 10.2 Read in CSV files Using read_csv(): Example: # Data Source: https://www.eia.gov/petroleum/supply/weekly/ library(readr) # Original code: PetroStocksData &lt;- readr::read_csv(&quot;data/PetroStocks.csv&quot;) names(PetroStocksData) &lt;- PetroStocksData[2,] # assign names to columns PetroStocksData &lt;- PetroStocksData[-1,] # remove first unused row PetroStocksData &lt;- PetroStocksData[-1,] # remove another unused row 10.2.1 Declaring data types Declaring data types as you read in data. Choose your data types! Reference: R4DS Ch. 15, 16 ### Revised using readr::read_csv() to skip empty rows and declare data types ### Declare column types within the read_csv() function call, to save yourself trouble later: ps_tbl &lt;- readr::read_csv(&quot;data/PetroStocks.csv&quot;, skip = 2, col_types = cols(.default = col_double(), &quot;Date&quot; = col_character())) ### Reference: https://readr.tidyverse.org/reference/read_delim.html "],["data-cleaning.html", "Chapter 11 Data cleaning 11.1 Dropping empty rows and columns", " Chapter 11 Data cleaning 11.1 Dropping empty rows and columns # Original: empty column identified manually PetroStocksData &lt;- PetroStocksData[,-21] # Delete unused last column ### Revised: This code removes *all* empty columns: ps_tbl %&gt;% select_if(function(x) !all(is.na(x))) -&gt; ps_tbl # Original: empty row identified manually: PetroStocksData &lt;- PetroStocksData[-2004,] # Delete unused last row ### Revised: This code removes all empty rows: ps_tbl %&gt;% filter(!across(everything(), is.na)) -&gt; ps_tbl "],["transform-a-data-frame-to-tsibble-object.html", "Chapter 12 Transform a data frame to tsibble object 12.1 Time indexing 12.2 Running diagnostics on your tsibble", " Chapter 12 Transform a data frame to tsibble object Readings: FPP3, Section 2.1 Convert the data frame into a time series tsibble object. # install.packages(&quot;tsibble&quot;) library(tsibble) # Reference: https://tsibble.tidyverts.org/articles/intro-tsibble.html esales &lt;- arrow::read_feather(&quot;data/esales.feather&quot;) esales %&gt;% dplyr::select(date, sales_GWh = value) -&gt; esales_tbl esales_tbl %&gt;% as_tsibble(index = date) -&gt; elsales_tbl_ts print(elsales_tbl_ts) ## # A tsibble: 233 x 2 [1D] ## date sales_GWh ## &lt;date&gt; &lt;dbl&gt; ## 1 2001-01-01 9576. ## 2 2001-02-01 7820. ## 3 2001-03-01 8070. ## 4 2001-04-01 7153. ## 5 2001-05-01 7224. ## 6 2001-06-01 8264. ## 7 2001-07-01 8896. ## 8 2001-08-01 9404. ## 9 2001-09-01 7753. ## 10 2001-10-01 7272. ## # … with 223 more rows 12.1 Time indexing See R4DS ch. 16. Depending on how dates and times are recorded in your raw data, you may face more or less work to organize them into form(s) suitable as tsibble index variable. The lubridate and hms packages may be valuable. # install.packages(&quot;feasts&quot;), Reference: https://feasts.tidyverts.org/ library(feasts) elsales_tbl_ts %&gt;% mutate(Month = tsibble::yearmonth(date)) %&gt;% as_tsibble(index = Month) %&gt;% dplyr::select(Month,sales_GWh) -&gt; vaelsales_tbl_ts print(vaelsales_tbl_ts) ## # A tsibble: 233 x 2 [1M] ## Month sales_GWh ## &lt;mth&gt; &lt;dbl&gt; ## 1 2001 Jan 9576. ## 2 2001 Feb 7820. ## 3 2001 Mar 8070. ## 4 2001 Apr 7153. ## 5 2001 May 7224. ## 6 2001 Jun 8264. ## 7 2001 Jul 8896. ## 8 2001 Aug 9404. ## 9 2001 Sep 7753. ## 10 2001 Oct 7272. ## # … with 223 more rows 12.2 Running diagnostics on your tsibble ** Ideally, should have exactly one row (i.e., one vector of measured values) for each time interval (index) and each value of the key variables. – May not have any duplicates. – May have missing values 12.2.1 Duplicate values 12.2.2 Missing values 12.2.3 Irregular time intervals "],["saving-and-loading-data-files.html", "Chapter 13 Saving and loading data files 13.1 Fast file reading and writing: The arrow package", " Chapter 13 Saving and loading data files Your real analytic work can begin when you have a prepared, cleaned data file ready to load into memory. You probably don’t want to re-do all the data preparation steps each time you start work. If you are working with a small data file, then it is probably not a problem to re-run the code to prepare your data file: it only takes a second. But especially if you are working with a larger file, re-doing the data prep for each work session is a hassle. Instead, you may wish to separate your data preparation code into its own script, then save the resulting prepared file to disk. Then, when you sit down to do analytic work, you can load your prepared file directly into memory. 13.1 Fast file reading and writing: The arrow package For saving and reading larger files, I recommend using the feather format supported by the arrow package. Arrow’s functions read_feather() and write_feather() work much faster than the corresponding read-write functions in base R. Arrow provides good support for work with large files, and plays well with dplyr. "],["assignment-prepare-your-data.html", "Chapter 14 Assignment: Prepare your data 14.1 Generate at least one table or figure", " Chapter 14 Assignment: Prepare your data This assignment focuses on the steps needed to clean, organize, and prepare your data set for time series analysis. You are asked to: Read the data into R Organize the data into a tidy data frame. Clean the data and perform various data quality checks Convert this data frame into a tsibble object to render it ready for time series analysis. Generate at least one table or graph based on the data (more if you like). 14.1 Generate at least one table or figure This step simply confirms that you have completed the preparation process and that your data is ready to analyze. You could for example simply add a line of code: print([name_of_your_tsibble]) to generate a simple table. "],["exploratory-analysis-of-time-series-data.html", "Chapter 15 Exploratory analysis of time series data 15.1 Overview 15.2 Briefly characterize the dataset 15.3 Plot the time series 15.4 Sesaonal plots 15.5 Scatterplots", " Chapter 15 Exploratory analysis of time series data 15.1 Overview Readings: FPP3, Sections 2.2–2.6. TSDS, Section 3.10, Chapter 4 and Section 5.5 Topics: Time series plots. Trends. Seasonal (periodic) patterns. Cycles. Seasonal plots. Seasonal sub-series. Investigating relationships between two variables. Scatterplots. Correlation. Scatterplot matrices. Assignment: Explore your data. 15.2 Briefly characterize the dataset Provide a brief example of the data, showing how they are structured. Example: Monthly electricity sales for Virginia Previously we extracted monthly electricity sales data for Virginia from a remote database, converted the data frame into a tibble object, and saved the result to a file in feather format. library(arrow) esales &lt;- read_feather(&quot;data/esales.feather&quot;) print(esales) # print the data as a table ## # A tibble: 233 × 4 ## value date year month ## &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 8282. 2020-05-01 2020 5 ## 2 7839. 2020-04-01 2020 4 ## 3 8889. 2020-03-01 2020 3 ## 4 9368. 2020-02-01 2020 2 ## 5 9209. 2020-01-01 2020 1 ## 6 10038. 2019-12-01 2019 12 ## 7 9291. 2019-11-01 2019 11 ## 8 8757. 2019-10-01 2019 10 ## 9 9874. 2019-09-01 2019 9 ## 10 10912. 2019-08-01 2019 8 ## # … with 223 more rows summary(esales) # compute basic summary statistics about the data ## value date year month ## Min. : 7153 Min. :2001-01-01 Min. :2001 Min. : 1.000 ## 1st Qu.: 8200 1st Qu.:2005-11-01 1st Qu.:2005 1st Qu.: 3.000 ## Median : 9019 Median :2010-09-01 Median :2010 Median : 6.000 ## Mean : 9093 Mean :2010-08-31 Mean :2010 Mean : 6.425 ## 3rd Qu.: 9885 3rd Qu.:2015-07-01 3rd Qu.:2015 3rd Qu.: 9.000 ## Max. :11724 Max. :2020-05-01 Max. :2020 Max. :12.000 boxplot(esales) hist(esales$value, breaks=40) # Make a histogram of monthly sales 15.2.1 Examine subsets of the data # References: https://www.tidyverse.org/, https://dplyr.tidyverse.org/ # filter(data object, condition) : syntax for filter() command esales %&gt;% filter(year == 2019) %&gt;% filter(value &gt; 9000) %&gt;% print() (esales %&gt;% group_by(year) %&gt;% summarise(Total = sum(value)) -&gt; total_esales_by_year) esales %&gt;% mutate(sales_TWh = value/1000) %&gt;% dplyr::select(-value) # library(lubridate) # Make it easy to deal with dates esales %&gt;% filter(month==3) # These three lines of code ## # A tibble: 20 × 4 ## value date year month ## &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 8889. 2020-03-01 2020 3 ## 2 9466. 2019-03-01 2019 3 ## 3 9666. 2018-03-01 2018 3 ## 4 9372. 2017-03-01 2017 3 ## 5 8406. 2016-03-01 2016 3 ## 6 9435. 2015-03-01 2015 3 ## 7 9676. 2014-03-01 2014 3 ## 8 9506. 2013-03-01 2013 3 ## 9 8086. 2012-03-01 2012 3 ## 10 8688. 2011-03-01 2011 3 ## 11 8568. 2010-03-01 2010 3 ## 12 8926. 2009-03-01 2009 3 ## 13 8512. 2008-03-01 2008 3 ## 14 8632. 2007-03-01 2007 3 ## 15 8519. 2006-03-01 2006 3 ## 16 9125. 2005-03-01 2005 3 ## 17 8136. 2004-03-01 2004 3 ## 18 8108. 2003-03-01 2003 3 ## 19 7675. 2002-03-01 2002 3 ## 20 8070. 2001-03-01 2001 3 esales %&gt;% filter(month(date)==3) # all do ## # A tibble: 20 × 4 ## value date year month ## &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 8889. 2020-03-01 2020 3 ## 2 9466. 2019-03-01 2019 3 ## 3 9666. 2018-03-01 2018 3 ## 4 9372. 2017-03-01 2017 3 ## 5 8406. 2016-03-01 2016 3 ## 6 9435. 2015-03-01 2015 3 ## 7 9676. 2014-03-01 2014 3 ## 8 9506. 2013-03-01 2013 3 ## 9 8086. 2012-03-01 2012 3 ## 10 8688. 2011-03-01 2011 3 ## 11 8568. 2010-03-01 2010 3 ## 12 8926. 2009-03-01 2009 3 ## 13 8512. 2008-03-01 2008 3 ## 14 8632. 2007-03-01 2007 3 ## 15 8519. 2006-03-01 2006 3 ## 16 9125. 2005-03-01 2005 3 ## 17 8136. 2004-03-01 2004 3 ## 18 8108. 2003-03-01 2003 3 ## 19 7675. 2002-03-01 2002 3 ## 20 8070. 2001-03-01 2001 3 esales %&gt;% filter(lubridate::month(date)==3) # the same thing. ## # A tibble: 20 × 4 ## value date year month ## &lt;dbl&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 8889. 2020-03-01 2020 3 ## 2 9466. 2019-03-01 2019 3 ## 3 9666. 2018-03-01 2018 3 ## 4 9372. 2017-03-01 2017 3 ## 5 8406. 2016-03-01 2016 3 ## 6 9435. 2015-03-01 2015 3 ## 7 9676. 2014-03-01 2014 3 ## 8 9506. 2013-03-01 2013 3 ## 9 8086. 2012-03-01 2012 3 ## 10 8688. 2011-03-01 2011 3 ## 11 8568. 2010-03-01 2010 3 ## 12 8926. 2009-03-01 2009 3 ## 13 8512. 2008-03-01 2008 3 ## 14 8632. 2007-03-01 2007 3 ## 15 8519. 2006-03-01 2006 3 ## 16 9125. 2005-03-01 2005 3 ## 17 8136. 2004-03-01 2004 3 ## 18 8108. 2003-03-01 2003 3 ## 19 7675. 2002-03-01 2002 3 ## 20 8070. 2001-03-01 2001 3 # We don&#39;t have to keep the &#39;year&#39; and &#39;month&#39; column: can recover them if needed esales %&gt;% dplyr::select(date, sales_GWh = value) -&gt; esales_tbl print(esales_tbl) ## # A tibble: 233 × 2 ## date sales_GWh ## &lt;date&gt; &lt;dbl&gt; ## 1 2020-05-01 8282. ## 2 2020-04-01 7839. ## 3 2020-03-01 8889. ## 4 2020-02-01 9368. ## 5 2020-01-01 9209. ## 6 2019-12-01 10038. ## 7 2019-11-01 9291. ## 8 2019-10-01 8757. ## 9 2019-09-01 9874. ## 10 2019-08-01 10912. ## # … with 223 more rows 15.3 Plot the time series Ref: FPP3, Section 2.2 #Reference: https://ggplot2.tidyverse.org/ ggplot(data=esales, aes(x=date,y=value)) + geom_line() + xlab(&quot;Year&quot;) + ylab(&quot;Virginia monthly total electricity sales (GWh)&quot;) 15.4 Sesaonal plots Ref: FPP3, Sections 2.3, 2.4 15.4.1 Example: Virginia monthly electricity Recall how we readied these data: esales &lt;- arrow::read_feather(&quot;data/esales.feather&quot;) esales %&gt;% dplyr::select(date, sales_GWh = value) -&gt; esales_tbl esales_tbl %&gt;% as_tsibble(index = date) -&gt; elsales_tbl_ts print(elsales_tbl_ts) ## # A tsibble: 233 x 2 [1D] ## date sales_GWh ## &lt;date&gt; &lt;dbl&gt; ## 1 2001-01-01 9576. ## 2 2001-02-01 7820. ## 3 2001-03-01 8070. ## 4 2001-04-01 7153. ## 5 2001-05-01 7224. ## 6 2001-06-01 8264. ## 7 2001-07-01 8896. ## 8 2001-08-01 9404. ## 9 2001-09-01 7753. ## 10 2001-10-01 7272. ## # … with 223 more rows ### This plot won&#39;t work. Why not? # elsales_tbl_ts %&gt;% # feasts::gg_season(sales_GWh, labels = &quot;both&quot;) + ylab(&quot;Virginia electricity sales (GWh)&quot;) # install.packages(&quot;feasts&quot;), Reference: https://feasts.tidyverts.org/ library(feasts) elsales_tbl_ts %&gt;% mutate(Month = tsibble::yearmonth(date)) %&gt;% as_tsibble(index = Month) %&gt;% dplyr::select(Month,sales_GWh) -&gt; vaelsales_tbl_ts print(vaelsales_tbl_ts) ## # A tsibble: 233 x 2 [1M] ## Month sales_GWh ## &lt;mth&gt; &lt;dbl&gt; ## 1 2001 Jan 9576. ## 2 2001 Feb 7820. ## 3 2001 Mar 8070. ## 4 2001 Apr 7153. ## 5 2001 May 7224. ## 6 2001 Jun 8264. ## 7 2001 Jul 8896. ## 8 2001 Aug 9404. ## 9 2001 Sep 7753. ## 10 2001 Oct 7272. ## # … with 223 more rows # feasts::autoplot() is handy for quickly generating time series plots autoplot(vaelsales_tbl_ts, sales_GWh) + ylab(&quot;Virginia monthly total electricity sales (GWh)&quot;) + xlab(&quot;&quot;) # Leave horiz. axis label blank vaelsales_tbl_ts %&gt;% gg_season(sales_GWh, labels = &quot;both&quot;) + ylab(&quot;Virginia electricity sales (GWh)&quot;) 15.4.2 Example: Australian production # install.packages(&#39;tsibbledata&#39;) library(tsibbledata) aus_production ## # A tsibble: 218 x 7 [1Q] ## Quarter Beer Tobacco Bricks Cement Electricity Gas ## &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1956 Q1 284 5225 189 465 3923 5 ## 2 1956 Q2 213 5178 204 532 4436 6 ## 3 1956 Q3 227 5297 208 561 4806 7 ## 4 1956 Q4 308 5681 197 570 4418 6 ## 5 1957 Q1 262 5577 187 529 4339 5 ## 6 1957 Q2 228 5651 214 604 4811 7 ## 7 1957 Q3 236 5317 227 603 5259 7 ## 8 1957 Q4 320 6152 222 582 4735 6 ## 9 1958 Q1 272 5758 199 554 4608 5 ## 10 1958 Q2 233 5641 229 620 5196 7 ## # … with 208 more rows aus_production %&gt;% gg_season(Electricity) aus_production %&gt;% gg_season(Beer) 15.4.3 Seasonal subseries plots Ref: FPP3, Section 2.5 vaelsales_tbl_ts %&gt;% gg_subseries(sales_GWh) + labs( y = &quot;Sales (GWh)&quot;, title = &quot;Seasonal subseries plot: Virginia electricity sales&quot; ) 15.5 Scatterplots Readings: FPP Sect. 2.6 Investigating relationships between two variables. Scatterplots. Correlation. Scatterplot matrices. vic_elec ## # A tsibble: 52,608 x 5 [30m] &lt;Australia/Melbourne&gt; ## Time Demand Temperature Date Holiday ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2012-01-01 00:00:00 4383. 21.4 2012-01-01 TRUE ## 2 2012-01-01 00:30:00 4263. 21.0 2012-01-01 TRUE ## 3 2012-01-01 01:00:00 4049. 20.7 2012-01-01 TRUE ## 4 2012-01-01 01:30:00 3878. 20.6 2012-01-01 TRUE ## 5 2012-01-01 02:00:00 4036. 20.4 2012-01-01 TRUE ## 6 2012-01-01 02:30:00 3866. 20.2 2012-01-01 TRUE ## 7 2012-01-01 03:00:00 3694. 20.1 2012-01-01 TRUE ## 8 2012-01-01 03:30:00 3562. 19.6 2012-01-01 TRUE ## 9 2012-01-01 04:00:00 3433. 19.1 2012-01-01 TRUE ## 10 2012-01-01 04:30:00 3359. 19.0 2012-01-01 TRUE ## # … with 52,598 more rows summary(vic_elec) ## Time Demand Temperature ## Min. :2012-01-01 00:00:00 Min. :2858 Min. : 1.50 ## 1st Qu.:2012-09-30 22:52:30 1st Qu.:3969 1st Qu.:12.30 ## Median :2013-07-01 22:45:00 Median :4635 Median :15.40 ## Mean :2013-07-01 22:45:00 Mean :4665 Mean :16.27 ## 3rd Qu.:2014-04-01 23:37:30 3rd Qu.:5244 3rd Qu.:19.40 ## Max. :2014-12-31 23:30:00 Max. :9345 Max. :43.20 ## Date Holiday ## Min. :2012-01-01 Mode :logical ## 1st Qu.:2012-09-30 FALSE:51120 ## Median :2013-07-01 TRUE :1488 ## Mean :2013-07-01 ## 3rd Qu.:2014-04-01 ## Max. :2014-12-31 vic_elec %&gt;% filter(year(Time) == 2013) %&gt;% autoplot(Demand) + labs( y = &quot;Demand (GW)&quot;, title = &quot;Half-hourly electricity demand: Victoria&quot; ) vic_elec %&gt;% filter(year(Time) == 2013) %&gt;% autoplot(Temperature) + labs( y = &quot;Temperature (degrees Celsius)&quot;, title = &quot;Half-hourly temperatures: Melbourne, Australia&quot; ) vic_elec %&gt;% filter(year(Time) == 2013) %&gt;% ggplot(aes(x = Temperature, y = Demand)) + # geom_density2d() + geom_point(size=0.1, aes(colour=Holiday), alpha = 0.4) + labs(y = &quot;Demand (GW)&quot;, x = &quot;Temperature (degrees Celsius)&quot;) 15.5.1 A Scatterplot matrix vic_elec ## # A tsibble: 52,608 x 5 [30m] &lt;Australia/Melbourne&gt; ## Time Demand Temperature Date Holiday ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;lgl&gt; ## 1 2012-01-01 00:00:00 4383. 21.4 2012-01-01 TRUE ## 2 2012-01-01 00:30:00 4263. 21.0 2012-01-01 TRUE ## 3 2012-01-01 01:00:00 4049. 20.7 2012-01-01 TRUE ## 4 2012-01-01 01:30:00 3878. 20.6 2012-01-01 TRUE ## 5 2012-01-01 02:00:00 4036. 20.4 2012-01-01 TRUE ## 6 2012-01-01 02:30:00 3866. 20.2 2012-01-01 TRUE ## 7 2012-01-01 03:00:00 3694. 20.1 2012-01-01 TRUE ## 8 2012-01-01 03:30:00 3562. 19.6 2012-01-01 TRUE ## 9 2012-01-01 04:00:00 3433. 19.1 2012-01-01 TRUE ## 10 2012-01-01 04:30:00 3359. 19.0 2012-01-01 TRUE ## # … with 52,598 more rows boxplot(vic_elec$Temperature) # install.packages(&quot;GGally&quot;) vic_elec %&gt;% # mutate(Temperature = round(Temperature)) %&gt;% # pivot_wider(values_from=c(Demand,Temperature), names_from=Holiday) %&gt;% GGally::ggpairs(columns = 3:2) vic_elec %&gt;% mutate(Year = factor(year(Date))) %&gt;% dplyr::select(-c(Date, Holiday)) %&gt;% GGally::ggpairs(columns = 4:2) "],["time-series-decomposition.html", "Chapter 16 Time series decomposition 16.1 Producing forecasts 16.2 Example: GDP, several countries", " Chapter 16 Time series decomposition Readings: FPP3, Sections 3.1, 3.2 16.0.1 Example: Gross Domestic Product data library(tsibbledata) # Data sets package print(global_economy) ## # A tsibble: 15,150 x 9 [1Y] ## # Key: Country [263] ## Country Code Year GDP Growth CPI Imports Exports Population ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan AFG 1960 537777811. NA NA 7.02 4.13 8996351 ## 2 Afghanistan AFG 1961 548888896. NA NA 8.10 4.45 9166764 ## 3 Afghanistan AFG 1962 546666678. NA NA 9.35 4.88 9345868 ## 4 Afghanistan AFG 1963 751111191. NA NA 16.9 9.17 9533954 ## 5 Afghanistan AFG 1964 800000044. NA NA 18.1 8.89 9731361 ## 6 Afghanistan AFG 1965 1006666638. NA NA 21.4 11.3 9938414 ## 7 Afghanistan AFG 1966 1399999967. NA NA 18.6 8.57 10152331 ## 8 Afghanistan AFG 1967 1673333418. NA NA 14.2 6.77 10372630 ## 9 Afghanistan AFG 1968 1373333367. NA NA 15.2 8.90 10604346 ## 10 Afghanistan AFG 1969 1408888922. NA NA 15.0 10.1 10854428 ## # … with 15,140 more rows global_economy %&gt;% filter(Country==&quot;Sweden&quot;) %&gt;% print() ## # A tsibble: 58 x 9 [1Y] ## # Key: Country [1] ## Country Code Year GDP Growth CPI Imports Exports Population ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sweden SWE 1960 14842870293. NA 9.21 23.4 23.0 7484656 ## 2 Sweden SWE 1961 16147160123. 5.68 9.41 21.7 22.3 7519998 ## 3 Sweden SWE 1962 17511477311. 4.26 9.86 21.4 21.9 7561588 ## 4 Sweden SWE 1963 18954132366. 5.33 10.1 21.5 21.9 7604328 ## 5 Sweden SWE 1964 21137242561. 6.82 10.5 21.9 22.3 7661354 ## 6 Sweden SWE 1965 23260320646. 3.82 11.0 22.5 21.9 7733853 ## 7 Sweden SWE 1966 25302033132. 2.09 11.7 21.9 21.4 7807797 ## 8 Sweden SWE 1967 27463409202. 3.37 12.2 21.0 21.1 7867931 ## 9 Sweden SWE 1968 29143383491. 3.64 12.5 21.6 21.6 7912273 ## 10 Sweden SWE 1969 31649203886. 5.01 12.8 23.0 22.8 7968072 ## # … with 48 more rows global_economy %&gt;% filter(Country==&quot;Sweden&quot;) %&gt;% autoplot(GDP) + ggtitle(&quot;GDP for Sweden&quot;) + ylab(&quot;$US billions&quot;) 16.0.2 Fitting data to simple models global_economy %&gt;% model(trend_model = TSLM(GDP ~ trend())) -&gt; fit fit ## # A mable: 263 x 2 ## # Key: Country [263] ## Country trend_model ## &lt;fct&gt; &lt;model&gt; ## 1 Afghanistan &lt;TSLM&gt; ## 2 Albania &lt;TSLM&gt; ## 3 Algeria &lt;TSLM&gt; ## 4 American Samoa &lt;TSLM&gt; ## 5 Andorra &lt;TSLM&gt; ## 6 Angola &lt;TSLM&gt; ## 7 Antigua and Barbuda &lt;TSLM&gt; ## 8 Arab World &lt;TSLM&gt; ## 9 Argentina &lt;TSLM&gt; ## 10 Armenia &lt;TSLM&gt; ## # … with 253 more rows fit %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() ## # A tsibble: 58 x 4 [1Y] ## # Key: Country, .model [1] ## Country .model Year .resid ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sweden trend_model 1960 79973991821. ## 2 Sweden trend_model 1961 71110300270. ## 3 Sweden trend_model 1962 62306636078. ## 4 Sweden trend_model 1963 53581309752. ## 5 Sweden trend_model 1964 45596438566. ## 6 Sweden trend_model 1965 37551535271. ## 7 Sweden trend_model 1966 29425266377. ## 8 Sweden trend_model 1967 21418661066. ## 9 Sweden trend_model 1968 12930653974. ## 10 Sweden trend_model 1969 5268492989. ## # … with 48 more rows fit %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() %&gt;% autoplot(.resid) 16.0.3 Work with ln(GDP) global_economy %&gt;% filter(Country==&quot;Sweden&quot;) %&gt;% autoplot(log(GDP)) + ggtitle(&quot;ln(GDP) for Sweden&quot;) + ylab(&quot;$US billions&quot;) global_economy %&gt;% model(trend_model = TSLM(log(GDP) ~ trend())) -&gt; logfit logfit %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() %&gt;% autoplot() global_economy %&gt;% model(trend_model = TSLM(log(GDP) ~ log(Population))) -&gt; fit3 fit3 %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() %&gt;% autoplot() 16.1 Producing forecasts fit %&gt;% forecast(h = &quot;3 years&quot;) -&gt; fcast3yrs fcast3yrs ## # A fable: 789 x 5 [1Y] ## # Key: Country, .model [263] ## Country .model Year GDP .mean ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dist&gt; &lt;dbl&gt; ## 1 Afghanistan trend_model 2018 N(1.6e+10, 1.3e+19) 16205101654. ## 2 Afghanistan trend_model 2019 N(1.7e+10, 1.3e+19) 16511878141. ## 3 Afghanistan trend_model 2020 N(1.7e+10, 1.3e+19) 16818654627. ## 4 Albania trend_model 2018 N(1.4e+10, 3.9e+18) 13733734164. ## 5 Albania trend_model 2019 N(1.4e+10, 3.9e+18) 14166852711. ## 6 Albania trend_model 2020 N(1.5e+10, 3.9e+18) 14599971258. ## 7 Algeria trend_model 2018 N(1.6e+11, 9.4e+20) 157895153441. ## 8 Algeria trend_model 2019 N(1.6e+11, 9.4e+20) 161100952126. ## 9 Algeria trend_model 2020 N(1.6e+11, 9.4e+20) 164306750811. ## 10 American Samoa trend_model 2018 N(6.8e+08, 1.7e+15) 682475000 ## # … with 779 more rows fcast3yrs %&gt;% filter(Country == &quot;Sweden&quot;, Year == 2020) %&gt;% str() ## fbl_ts [1 × 5] (S3: fbl_ts/tbl_ts/tbl_df/tbl/data.frame) ## $ Country: Factor w/ 263 levels &quot;Afghanistan&quot;,..: 232 ## $ .model : chr &quot;trend_model&quot; ## $ Year : num 2020 ## $ GDP : dist [1:1] ## ..$ 3:List of 2 ## .. ..$ mu : num 5.45e+11 ## .. ..$ sigma: num 5.34e+10 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;dist_normal&quot; &quot;dist_default&quot; ## ..@ vars: chr &quot;GDP&quot; ## $ .mean : num 5.45e+11 ## - attr(*, &quot;key&quot;)= tibble [1 × 3] (S3: tbl_df/tbl/data.frame) ## ..$ Country: Factor w/ 263 levels &quot;Afghanistan&quot;,..: 232 ## ..$ .model : chr &quot;trend_model&quot; ## ..$ .rows : list&lt;int&gt; [1:1] ## .. ..$ : int 1 ## .. ..@ ptype: int(0) ## ..- attr(*, &quot;.drop&quot;)= logi TRUE ## - attr(*, &quot;index&quot;)= chr &quot;Year&quot; ## ..- attr(*, &quot;ordered&quot;)= logi TRUE ## - attr(*, &quot;index2&quot;)= chr &quot;Year&quot; ## - attr(*, &quot;interval&quot;)= interval [1:1] 1Y ## ..@ .regular: logi TRUE ## - attr(*, &quot;response&quot;)= chr &quot;GDP&quot; ## - attr(*, &quot;dist&quot;)= chr &quot;GDP&quot; ## - attr(*, &quot;model_cn&quot;)= chr &quot;.model&quot; fcast3yrs %&gt;% filter(Country==&quot;Sweden&quot;) %&gt;% autoplot(global_economy) + ggtitle(&quot;GDP for Sweden&quot;) + ylab(&quot;$US billions&quot;) 16.1.1 Model residuals vs. forecast errors Model residuals: Your data: \\(y_1, y_2, \\ldots, y_T\\) Fitted values: \\(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_T\\) Model residuals: \\(e_t = y_t - \\hat{y}_t\\) Forecast errors: augment(fit) ## # A tsibble: 15,150 x 7 [1Y] ## # Key: Country, .model [263] ## Country .model Year GDP .fitted .resid .innov ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan trend_model 1960 537777811. -1587934559. 2125712370. 2.13e9 ## 2 Afghanistan trend_model 1961 548888896. -1281158073. 1830046968. 1.83e9 ## 3 Afghanistan trend_model 1962 546666678. -974381586. 1521048264. 1.52e9 ## 4 Afghanistan trend_model 1963 751111191. -667605100. 1418716291. 1.42e9 ## 5 Afghanistan trend_model 1964 800000044. -360828613. 1160828658. 1.16e9 ## 6 Afghanistan trend_model 1965 1006666638. -54052127. 1060718765. 1.06e9 ## 7 Afghanistan trend_model 1966 1399999967. 252724359. 1147275607. 1.15e9 ## 8 Afghanistan trend_model 1967 1673333418. 559500846. 1113832572. 1.11e9 ## 9 Afghanistan trend_model 1968 1373333367. 866277332. 507056034. 5.07e8 ## 10 Afghanistan trend_model 1969 1408888922. 1173053819. 235835103. 2.36e8 ## # … with 15,140 more rows augment(fit) %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% ggplot(aes(x = .resid)) + geom_histogram(bins = 20) + ggtitle(&quot;Histogram of residuals&quot;) 16.1.2 Are the model residuals auto-correlated? augment(fit) %&gt;% filter(Country == &quot;Sweden&quot;) -&gt; augSweden augSweden %&gt;% ACF(.resid) %&gt;% autoplot() + ggtitle(&quot;ACF of residuals&quot;) augment(fit3) %&gt;% filter(Country == &quot;Sweden&quot;) -&gt; augSweden3 augSweden3 %&gt;% ACF(.resid) %&gt;% autoplot() + ggtitle(&quot;ACF of residuals&quot;) 16.2 Example: GDP, several countries library(tsibbledata) # Data sets package nordic &lt;- c(&quot;Sweden&quot;, &quot;Denmark&quot;, &quot;Norway&quot;, &quot;Finland&quot;) (global_economy %&gt;% filter(Country %in% nordic) -&gt; nordic_economy) ## # A tsibble: 232 x 9 [1Y] ## # Key: Country [4] ## Country Code Year GDP Growth CPI Imports Exports Population ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark DNK 1960 6248946880. NA 8.25 34.3 32.3 4579603 ## 2 Denmark DNK 1961 6933842099. 6.38 8.53 32.3 30.0 4611687 ## 3 Denmark DNK 1962 7812968114. 5.67 9.16 32.5 28.6 4647727 ## 4 Denmark DNK 1963 8316692386. 0.637 9.72 30.8 30.4 4684483 ## 5 Denmark DNK 1964 9506678763. 9.27 10.0 32.6 29.9 4722072 ## 6 Denmark DNK 1965 10678897387. 4.56 10.6 31.5 29.3 4759012 ## 7 Denmark DNK 1966 11721248101. 2.74 11.3 30.8 28.6 4797381 ## 8 Denmark DNK 1967 12788479692. 3.42 12.2 30.0 27.3 4835354 ## 9 Denmark DNK 1968 13196541952 3.97 13.2 29.7 27.7 4864883 ## 10 Denmark DNK 1969 15009384585. 6.32 13.7 30.4 27.6 4891860 ## # … with 222 more rows nordic_economy %&gt;% autoplot(GDP) fitnord &lt;- nordic_economy %&gt;% model( trend_model = TSLM(GDP ~ trend()), trend_model_ln = TSLM(log(GDP) ~ trend()), ets = ETS(GDP ~ trend(&quot;A&quot;)), arima = ARIMA(GDP) ) fitnord ## # A mable: 4 x 5 ## # Key: Country [4] ## Country trend_model trend_model_ln ets arima ## &lt;fct&gt; &lt;model&gt; &lt;model&gt; &lt;model&gt; &lt;model&gt; ## 1 Denmark &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(1,1,1)&gt; ## 2 Finland &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(0,1,2)&gt; ## 3 Norway &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(0,1,1)&gt; ## 4 Sweden &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(0,1,2)&gt; fitnord %&gt;% dplyr::select(arima) %&gt;% coef() ## # A tibble: 7 × 7 ## Country .model term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark arima ar1 -0.390 0.206 -1.89 0.0636 ## 2 Denmark arima ma1 0.724 0.143 5.05 0.00000484 ## 3 Finland arima ma1 0.406 0.120 3.39 0.00126 ## 4 Finland arima ma2 -0.221 0.108 -2.05 0.0450 ## 5 Norway arima ma1 0.410 0.155 2.65 0.0104 ## 6 Sweden arima ma1 0.241 0.121 1.99 0.0510 ## 7 Sweden arima ma2 -0.188 0.101 -1.87 0.0670 Denmark: ARMA(1,1) Finland: MA(2) Norway: MA(1) Sweden: MA(2) nordic_economy %&gt;% model(arima_constrained = ARIMA(GDP ~ pdq(1,0,2))) %&gt;% dplyr::select(arima_constrained) %&gt;% coef() ## # A tibble: 0 × 4 ## # … with 4 variables: Country &lt;fct&gt;, .model &lt;chr&gt;, term &lt;chr&gt;, estimate &lt;dbl&gt; fitnord %&gt;% coef() ## # A tibble: 39 × 7 ## Country .model term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark trend_model (Intercept) -5.65e+10 8.75e+9 -6.46 2.70e- 8 ## 2 Denmark trend_model trend() 6.63e+ 9 2.58e+8 25.7 1.14e-32 ## 3 Denmark trend_model_ln (Intercept) 2.30e+ 1 8.55e-2 269. 7.68e-89 ## 4 Denmark trend_model_ln trend() 7.12e- 2 2.52e-3 28.3 7.68e-35 ## 5 Denmark ets alpha 1.00e+ 0 NA NA NA ## 6 Denmark ets beta 3.67e- 1 NA NA NA ## 7 Denmark ets l[0] 4.92e+ 9 NA NA NA ## 8 Denmark ets b[0] 1.24e+ 9 NA NA NA ## 9 Denmark arima ar1 -3.90e- 1 2.06e-1 -1.89 6.36e- 2 ## 10 Denmark arima ma1 7.24e- 1 1.43e-1 5.05 4.84e- 6 ## # … with 29 more rows fitnord %&gt;% glance() ## # A tibble: 16 × 21 ## Country .model r_squared adj_r_squared sigma2 statistic p_value df ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Denmark trend_mod… 0.922 0.920 1.08e+21 660. 1.14e-32 2 ## 2 Denmark trend_mod… 0.935 0.933 1.03e- 1 800. 7.68e-35 2 ## 3 Denmark ets NA NA 1.04e- 2 NA NA NA ## 4 Denmark arima NA NA 2.41e+20 NA NA NA ## 5 Finland trend_mod… 0.914 0.912 7.34e+20 594. 1.70e-31 2 ## 6 Finland trend_mod… 0.930 0.929 1.14e- 1 745. 4.96e-34 2 ## 7 Finland ets NA NA 1.32e- 2 NA NA NA ## 8 Finland arima NA NA 1.89e+20 NA NA NA ## 9 Norway trend_mod… 0.824 0.821 4.60e+21 262. 8.54e-23 2 ## 10 Norway trend_mod… 0.959 0.958 8.37e- 2 1307. 1.64e-40 2 ## 11 Norway ets NA NA 8.23e- 3 NA NA NA ## 12 Norway arima NA NA 6.78e+20 NA NA NA ## 13 Sweden trend_mod… 0.919 0.918 2.65e+21 635. 3.07e-32 2 ## 14 Sweden trend_mod… 0.935 0.933 8.19e- 2 800. 7.57e-35 2 ## 15 Sweden ets NA NA 1.16e- 2 NA NA NA ## 16 Sweden arima NA NA 8.84e+20 NA NA NA ## # … with 13 more variables: log_lik &lt;dbl&gt;, AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, ## # CV &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt;, MSE &lt;dbl&gt;, ## # AMSE &lt;dbl&gt;, MAE &lt;dbl&gt;, ar_roots &lt;list&gt;, ma_roots &lt;list&gt; fitnord %&gt;% filter(Country == &quot;Denmark&quot;) %&gt;% dplyr::select(arima) %&gt;% report() ## Series: GDP ## Model: ARIMA(1,1,1) ## ## Coefficients: ## ar1 ma1 ## -0.3898 0.7240 ## s.e. 0.2061 0.1434 ## ## sigma^2 estimated as 2.407e+20: log likelihood=-1417.5 ## AIC=2840.99 AICc=2841.45 BIC=2847.12 fitnord %&gt;% accuracy() %&gt;% arrange(Country, MPE) ## # A tibble: 16 × 11 ## Country .model .type ME RMSE MAE MPE MAPE MASE RMSSE ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark trend_mod… Trai… -1.12e+10 6.89e10 3.67e10 -5.17 28.0 3.34 4.24 ## 2 Denmark ets Trai… 4.50e+ 7 1.65e10 1.04e10 0.518 7.09 0.946 1.02 ## 3 Denmark arima Trai… 4.40e+ 9 1.51e10 1.04e10 5.05 8.16 0.945 0.930 ## 4 Denmark trend_mod… Trai… -2.06e- 6 3.23e10 2.63e10 51.1 80.8 2.40 1.99 ## 5 Finland trend_mod… Trai… -8.61e+ 9 5.64e10 2.99e10 -5.53 28.6 2.95 3.82 ## 6 Finland ets Trai… 1.36e+ 8 1.47e10 9.41e 9 0.795 8.36 0.927 0.996 ## 7 Finland arima Trai… 3.54e+ 9 1.34e10 9.14e 9 5.03 8.92 0.900 0.906 ## 8 Finland trend_mod… Trai… 9.54e- 7 2.66e10 2.21e10 46.1 80.5 2.18 1.80 ## 9 Norway trend_mod… Trai… -1.31e+10 8.20e10 3.51e10 -4.24 24.9 2.24 3.01 ## 10 Norway ets Trai… -5.29e+ 8 2.75e10 1.37e10 0.755 6.94 0.870 1.01 ## 11 Norway arima Trai… 4.90e+ 9 2.56e10 1.40e10 5.04 8.11 0.890 0.938 ## 12 Norway trend_mod… Trai… 2.09e- 6 6.67e10 5.48e10 130. 181. 3.49 2.45 ## 13 Sweden trend_mod… Trai… -1.18e+10 8.23e10 4.79e10 -3.96 23.7 2.25 2.68 ## 14 Sweden ets Trai… 1.19e+ 9 3.02e10 1.86e10 0.745 7.64 0.875 0.984 ## 15 Sweden arima Trai… 8.48e+ 9 2.89e10 2.01e10 5.18 9.37 0.942 0.944 ## 16 Sweden trend_mod… Trai… 2.10e- 6 5.05e10 3.90e10 29.4 53.3 1.83 1.65 ## # … with 1 more variable: ACF1 &lt;dbl&gt; # ETS forecasts USAccDeaths %&gt;% ets() %&gt;% forecast() %&gt;% autoplot() str(taylor) plot(taylor) 16.2.1 Plot lagged values vaelsales_tbl_ts %&gt;% filter(month(Month) %in% c(3,6,9,12)) %&gt;% gg_lag(sales_GWh, lags = 1:2) vaelsales_tbl_ts %&gt;% filter(month(Month) == 1) %&gt;% gg_lag(sales_GWh, lags = 1:2) vaelsales_tbl_ts %&gt;% ACF(sales_GWh) %&gt;% autoplot() # decompose(vaelsales_tbl_ts) vaelsales_tbl_ts %&gt;% model(STL(sales_GWh ~ trend(window=21) + season(window=&#39;periodic&#39;), robust = TRUE)) %&gt;% components() %&gt;% autoplot() vaelsales_tbl_ts %&gt;% mutate(ln_sales_GWh = log(sales_GWh)) %&gt;% model(STL(ln_sales_GWh ~ trend(window=21) + season(window=&#39;periodic&#39;), robust = TRUE)) %&gt;% components() %&gt;% autoplot() vaelsales_tbl_ts %&gt;% features(sales_GWh, feat_stl) ## # A tibble: 1 × 9 ## trend_strength seasonal_strength_… seasonal_peak_y… seasonal_trough… spikiness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.609 0.869 7 4 747395. ## # … with 4 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, ## # stl_e_acf10 &lt;dbl&gt; vaelsales_tbl_ts %&gt;% features(sales_GWh, feature_set(pkgs=&quot;feasts&quot;)) ## # A tibble: 1 × 47 ## trend_strength seasonal_strength_… seasonal_peak_y… seasonal_trough… spikiness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.609 0.869 7 4 747395. ## # … with 42 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, ## # stl_e_acf10 &lt;dbl&gt;, acf1 &lt;dbl&gt;, acf10 &lt;dbl&gt;, diff1_acf1 &lt;dbl&gt;, ## # diff1_acf10 &lt;dbl&gt;, diff2_acf1 &lt;dbl&gt;, diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;, ## # pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;, diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;, ## # zero_run_mean &lt;dbl&gt;, nonzero_squared_cv &lt;dbl&gt;, zero_start_prop &lt;dbl&gt;, ## # zero_end_prop &lt;dbl&gt;, lambda_guerrero &lt;dbl&gt;, kpss_stat &lt;dbl&gt;, ## # kpss_pvalue &lt;dbl&gt;, pp_stat &lt;dbl&gt;, pp_pvalue &lt;dbl&gt;, ndiffs &lt;int&gt;, … "],["autocorrelation.html", "Chapter 17 Autocorrelation 17.1 Heere be monsters", " Chapter 17 Autocorrelation Readings: FPP3 Sections 2.7-2.9, 4.2, 4.5 vaelsales_tbl_ts %&gt;% filter(month(Month) %in% c(3,6,9,12)) %&gt;% gg_lag(sales_GWh, lags = 1:2) vaelsales_tbl_ts %&gt;% filter(month(Month) == 1) %&gt;% gg_lag(sales_GWh, lags = 1:2) vaelsales_tbl_ts %&gt;% ACF(sales_GWh) %&gt;% autoplot() 17.1 Heere be monsters knitr::include_graphics(&quot;graphics/horst_acf/horst_acf_1.jpg&quot;) "],["the-data-generating-process.html", "Chapter 18 The data-generating process 18.1 The white noise process", " Chapter 18 The data-generating process In Section 9.4.2 we introduced a structure for a typical table of time series data: Date Series Value_1 Value_2 Value_3 2020-02-01 “Virginia” 33.57 29 “friendly” 2020-02-01 “Idaho” 0.22 18 “hostile” … … … … … index key [date] [fctr] [dbl] [int] [fctr] Here, the Date field contains values for regular time intervals on which data are recorded. The fields Value_1, Value_2, etc., contain the actual observed values. The Series field reports objects (here, U.S. states) for which data are reported. Let’s make this setup both simpler and somewhat more general. For now, let’s drop the key field Series, and suppose we are dealing only with one time series. Let’s abstract from the Date field, and just label the sequence of time steps \\(t = 1,2,\\ldots,T\\). Let’s suppose further we have only one sequence of data values \\(y_1, y_2, \\ldots, y_T\\). Our abstracted data table now looks like: \\(t\\) \\(y\\) \\(1\\) \\(y_1\\) \\(2\\) \\(y_2\\) … … \\(T\\) \\(y_T\\) Suppose we have a dataset structured in this way: ## # A tibble: 40 × 2 ## t y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 16.1 ## 2 2 18.6 ## 3 3 15.5 ## 4 4 22.8 ## 5 5 19.0 ## 6 6 15.5 ## 7 7 19.5 ## 8 8 20.2 ## 9 9 19.7 ## 10 10 17.1 ## # … with 30 more rows Given such a dataset, the challenge we confront is how best to explain the process by which the observed data were generated, and also to predict future values from this process that haven’t been observed yet. To meet this challenge, the central step is to create a formal mathematical model of the data-generating process. To explain what that means, we’ll start with a simple example. 18.1 The white noise process One of the simplest models of a process for generating these data is to treat them as white noise. We actually introduced this model previously, in Section 2.5. In a white noise process, the data are generated as independent, identically distributed random draws from a fixed normal distribution. Formally, for \\(t = 1, \\ldots, T\\), \\[ y_t = \\theta + \\varepsilon_t \\] where \\(\\theta\\) denotes the mean value of the process, and where \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\) is a random noise term. Here, the parameters \\(\\theta\\) and \\(\\sigma^2\\) are assumed to be constant. The values of these parameters are not directly observable. Instead, their values must be estimated based on the observed data, using established techniques of statistical estimation. Since the data we have are limited and noisy, these estimates will in general be only approximately correct. The issue for practical work is whether these approximations are close enough to be useful in our specific application. Before we get into statistical estimation techniques, let’s take up a prior question: Why, or when, should we believe this model? What would make us think this model is a true — or at least, serviceably close-enough — representation of the underlying process that generated the observed data? A reasonable approach is to evaluate whether the data “look like” what we would expect to see, if the model were true. If the white noise model were the true model of the data-generating process, then several observable features in the data should be apparent. 18.1.1 Stationarity First, the process would be stationary. Formally, a stochastic process is stationary if its unconditional joint probability distribution does not change when shifted in time. There should be no discernible upward or downward trend over time, for example. There are (of course) statistical tests for stationarity one can apply to a time series to estimate the likelihood that the series (more exactly: the underlying stochastic process that generated the series) is stationary. But before applying such tests, it’s a good idea to just plot the data and ask: do they look stationary? library(ggplot2) library(ggthemes) p &lt;- ggplot(y_tbl, aes(t, y)) + geom_point() + xlab(expression(t)) + ylab(expression(y[t])) + theme_clean() p Just by visual inspection, there doesn’t seem to be much of a trend. Fitting a linear model to the data and plotting the regression line reveals a very shallow downward trend: p + geom_smooth(method = &quot;lm&quot;, se = FALSE) Hmmm. Is this trend “real”? Or did it just happen that, by random chance, the data later in the series happen to have slightly lower values on average than those generated earlier? This question is very typical of those that arise in the challenge of model selection, i.e., of choosing the model that best explains the data generating process. It is sometimes difficult to tell whether a pattern we observe in our data corresponds to a real feature of the underlying process, or if the feature is instead a kind of illusion — one that just happened to emerge from the particular data we observe, due to random chance, but that would not necessarily be observed from another sample of data drawn from the same process. We could (with caution) test the statistical significance of the trend term: y_tbl %&gt;% lm(y ~ t, data = .) -&gt; temp.lm print(temp.lm) ## ## Call: ## lm(formula = y ~ t, data = .) ## ## Coefficients: ## (Intercept) t ## 18.50754 -0.01129 coef(temp.lm) ## (Intercept) t ## 18.50754230 -0.01129092 18.1.2 No autocorrelation Temperatures are assumed to be independent from one year to the next. In particular, there is no autocorrelation. Knowing that one year’s temperature was unusually high (say) provides no information about the likelihood that next year’s temperature will also be unusually high. Inter-annual climate cycles (e.g., due to \\(El\\ Ni\\tilde{n}o\\)) are ruled out. Temperature variations around the long-run average are assumed to be identically distributed. This assumption rules out the possibility that variance is, say, greater when temperatures are higher than when they are lower. Nor do they grow over time. "],["the-normal-linear-model.html", "Chapter 19 The normal linear model 19.1 Assumptions of the linear model 19.2 Examples of the normal linear model", " Chapter 19 The normal linear model \\[ y_t = \\beta_0 + \\beta_1 x_t + \\varepsilon_t \\] 19.1 Assumptions of the linear model Relationship between predictor \\(x\\) and predictand \\(y\\) is linear. Both \\(x\\) and \\(y\\) are known, observed without error. Errors have mean zero. Errors are independent of each other. Errors are uncorrelated with predictor variables \\(x_t\\). Often, assume stronger additional conditions that errors are independent, identically normally distributed: for all \\(t\\), \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\). for a constant \\(\\sigma^2\\). In compact vector and matrix notation, we may write: \\[ Y = X \\beta + \\varepsilon, \\quad \\text{where $\\varepsilon \\sim N(0, \\sigma^2 I_T)$} \\] Readings: FPP, Section 7.1 19.2 Examples of the normal linear model "],["ordinary-least-squares-estimation.html", "Chapter 20 Ordinary least squares estimation", " Chapter 20 Ordinary least squares estimation Regression coefficients \\(\\beta\\) and error variance \\(\\sigma^2\\) are unobserved: their values must be estimated from the data. Various estimation techniques may be used. "],["assignment-project-proposal.html", "Chapter 21 Assignment: Project proposal 21.1 Introduction 21.2 The data and the data-generating process 21.3 Exploratory data analysis 21.4 Plot the time series. 21.5 Perform and report the results of other exploratory data analysis 21.6 Statistical model 21.7 Plan for data analysis 21.8 Submission requirements 21.9 Comment 21.10 References", " Chapter 21 Assignment: Project proposal In this assignment you will develop your initial concept note into a draft of a full project proposal. Treat this assignment as a “dry run” for developing a proposal for a grant or fellowship application, or for your Ph.D. prospectus. Your proposal should include at least the following sections and information. Front matter: Descriptive title, your name, date, reference to “SYS 5581 Time Series &amp; Forecasting, Spring 2021”. Abstract: A very brief summary of the project. 21.1 Introduction Give a narrative description of the problem you are addressing, and the methods you will use to address it. Provide context: What is the question you are attempting to answer? Why is this question important? (Who cares?) How will you go about attempting to answer this question? This work addresses the question: Why do people not use probabilistic forecasts for decision-making? 21.2 The data and the data-generating process Describe the data set you will be analyzing, and where it comes from, how it was generated and collected. Identify the source of the data. Give a narrative description of the data-generating process: this piece is critical. Since these will be time series data: identify the frequency of the data series (e.g., hourly, monthly), and the period of record. 21.3 Exploratory data analysis Provide a brief example of the data, showing how they are structured. 21.4 Plot the time series. 21.5 Perform and report the results of other exploratory data analysis 21.5.1 STL decomposition vaelsales_tbl_ts %&gt;% model(STL(sales_GWh ~ trend(window=21) + season(window=&#39;periodic&#39;), robust = TRUE)) %&gt;% components() %&gt;% autoplot() vaelsales_tbl_ts %&gt;% mutate(ln_sales_GWh = log(sales_GWh)) %&gt;% model(STL(ln_sales_GWh ~ trend(window=21) + season(window=&#39;periodic&#39;), robust = TRUE)) %&gt;% components() %&gt;% autoplot() vaelsales_tbl_ts %&gt;% features(sales_GWh, feat_stl) ## # A tibble: 1 × 9 ## trend_strength seasonal_strength_… seasonal_peak_y… seasonal_trough… spikiness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.609 0.869 7 4 747395. ## # … with 4 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, ## # stl_e_acf10 &lt;dbl&gt; vaelsales_tbl_ts %&gt;% features(sales_GWh, feature_set(pkgs=&quot;feasts&quot;)) ## # A tibble: 1 × 47 ## trend_strength seasonal_strength_… seasonal_peak_y… seasonal_trough… spikiness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.609 0.869 7 4 747395. ## # … with 42 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, ## # stl_e_acf10 &lt;dbl&gt;, acf1 &lt;dbl&gt;, acf10 &lt;dbl&gt;, diff1_acf1 &lt;dbl&gt;, ## # diff1_acf10 &lt;dbl&gt;, diff2_acf1 &lt;dbl&gt;, diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;, ## # pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;, diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;, ## # zero_run_mean &lt;dbl&gt;, nonzero_squared_cv &lt;dbl&gt;, zero_start_prop &lt;dbl&gt;, ## # zero_end_prop &lt;dbl&gt;, lambda_guerrero &lt;dbl&gt;, kpss_stat &lt;dbl&gt;, ## # kpss_pvalue &lt;dbl&gt;, pp_stat &lt;dbl&gt;, pp_pvalue &lt;dbl&gt;, ndiffs &lt;int&gt;, … 21.5.2 Fitting data to simple models global_economy %&gt;% model(trend_model = TSLM(GDP ~ trend())) -&gt; fit fit ## # A mable: 263 x 2 ## # Key: Country [263] ## Country trend_model ## &lt;fct&gt; &lt;model&gt; ## 1 Afghanistan &lt;TSLM&gt; ## 2 Albania &lt;TSLM&gt; ## 3 Algeria &lt;TSLM&gt; ## 4 American Samoa &lt;TSLM&gt; ## 5 Andorra &lt;TSLM&gt; ## 6 Angola &lt;TSLM&gt; ## 7 Antigua and Barbuda &lt;TSLM&gt; ## 8 Arab World &lt;TSLM&gt; ## 9 Argentina &lt;TSLM&gt; ## 10 Armenia &lt;TSLM&gt; ## # … with 253 more rows fit %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() ## # A tsibble: 58 x 4 [1Y] ## # Key: Country, .model [1] ## Country .model Year .resid ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sweden trend_model 1960 79973991821. ## 2 Sweden trend_model 1961 71110300270. ## 3 Sweden trend_model 1962 62306636078. ## 4 Sweden trend_model 1963 53581309752. ## 5 Sweden trend_model 1964 45596438566. ## 6 Sweden trend_model 1965 37551535271. ## 7 Sweden trend_model 1966 29425266377. ## 8 Sweden trend_model 1967 21418661066. ## 9 Sweden trend_model 1968 12930653974. ## 10 Sweden trend_model 1969 5268492989. ## # … with 48 more rows fit %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() %&gt;% autoplot(.resid) 21.5.3 Work with ln(GDP) global_economy %&gt;% filter(Country==&quot;Sweden&quot;) %&gt;% autoplot(log(GDP)) + ggtitle(&quot;ln(GDP) for Sweden&quot;) + ylab(&quot;$US billions&quot;) global_economy %&gt;% model(trend_model = TSLM(log(GDP) ~ trend())) -&gt; logfit logfit %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() %&gt;% autoplot() global_economy %&gt;% model(trend_model = TSLM(log(GDP) ~ log(Population))) -&gt; fit3 fit3 %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% residuals() %&gt;% autoplot() 21.6 Statistical model 21.6.1 Formal model of data-generating process Write down an equation (or set of equations) that represent the data-generating process formally. If applicable: describe any transformations of the data (e.g., differencing, taking logs) you need to make to get the data into a form (e.g., linear) ready for numerical analysis. What kind of process is it? \\(AR(p)\\)? White noise with drift? Something else? Write down an equation expressing each realization of the stochastic process \\(y_t\\) as a function of other observed data (which could include lagged values of \\(y\\)), unobserved parameters (\\(\\beta\\)), and an error term (\\(\\varepsilon_t\\)). Ex: \\[y = X\\cdot\\beta + \\varepsilon\\] Add a model of the error process. Ex: \\(\\varepsilon \\sim N(0, \\sigma^2 I_T)\\). 21.6.2 Discussion of the statistical model Describe how the formal statistical model captures and aligns with the narrative of the data-generating process. Flag any statistical challenges raised by the data generating process, e.g. selection bias; survivorship bias; omitted variables bias, etc. 21.7 Plan for data analysis Describe what information you wish to extract from the data. Do you wish to… estimate the values of the unobserved model parameters? create a tool for forecasting? estimate the exceedance probabilities for future realizations of \\(y_t\\)? Describe your plan for getting this information. OLS regression? Some other statistical technique? If you can: describe briefly which computational tools you will use (e.g., R), and which packages you expect to draw on. 21.8 Submission requirements Prepare your proposal using Markdown . (You may find it useful to generate your Markdown file from some other tool, e.g. R Markdown in R Studio.) Submit your proposal by pushing it to your repo within the course organization on Github. When your proposal is ready, notify the instructor by also creating a submission for this assignment on Collab. Please also upload a PDF version of your proposal to Collab as part of your submission. 21.9 Comment Depending on your prior experience, you may find this assignment challenging. Treat this assignment as an opportunity to make progress on your own research program. Make your proposal as complete as you can. But note that this assignment is merely the First Draft. You will have more opportunity to refine your work over the next two months, in consultation with the instructor, your advisor, and your classmates. 21.10 References "],["evaluating-model-residuals.html", "Chapter 22 Evaluating model residuals 22.1 Model residuals vs. forecast errors 22.2 Are the model residuals auto-correlated? 22.3 Example: GDP, several models, several countries", " Chapter 22 Evaluating model residuals 22.1 Model residuals vs. forecast errors Model residuals: Your data: \\(y_1, y_2, \\ldots, y_T\\) Fitted values: \\(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_T\\) Model residuals: \\(e_t = y_t - \\hat{y}_t\\) Forecast errors: augment(fit) ## # A tsibble: 15,150 x 7 [1Y] ## # Key: Country, .model [263] ## Country .model Year GDP .fitted .resid .innov ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan trend_model 1960 537777811. -1587934559. 2125712370. 2.13e9 ## 2 Afghanistan trend_model 1961 548888896. -1281158073. 1830046968. 1.83e9 ## 3 Afghanistan trend_model 1962 546666678. -974381586. 1521048264. 1.52e9 ## 4 Afghanistan trend_model 1963 751111191. -667605100. 1418716291. 1.42e9 ## 5 Afghanistan trend_model 1964 800000044. -360828613. 1160828658. 1.16e9 ## 6 Afghanistan trend_model 1965 1006666638. -54052127. 1060718765. 1.06e9 ## 7 Afghanistan trend_model 1966 1399999967. 252724359. 1147275607. 1.15e9 ## 8 Afghanistan trend_model 1967 1673333418. 559500846. 1113832572. 1.11e9 ## 9 Afghanistan trend_model 1968 1373333367. 866277332. 507056034. 5.07e8 ## 10 Afghanistan trend_model 1969 1408888922. 1173053819. 235835103. 2.36e8 ## # … with 15,140 more rows augment(fit) %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% ggplot(aes(x = .resid)) + geom_histogram(bins = 20) + ggtitle(&quot;Histogram of residuals&quot;) 22.2 Are the model residuals auto-correlated? augment(fit) %&gt;% filter(Country == &quot;Sweden&quot;) -&gt; augSweden augSweden %&gt;% ACF(.resid) %&gt;% autoplot() + ggtitle(&quot;ACF of residuals&quot;) augment(fit3) %&gt;% filter(Country == &quot;Sweden&quot;) -&gt; augSweden3 augSweden3 %&gt;% ACF(.resid) %&gt;% autoplot() + ggtitle(&quot;ACF of residuals&quot;) 22.3 Example: GDP, several models, several countries library(tsibbledata) # Data sets package nordic &lt;- c(&quot;Sweden&quot;, &quot;Denmark&quot;, &quot;Norway&quot;, &quot;Finland&quot;) (global_economy %&gt;% filter(Country %in% nordic) -&gt; nordic_economy) ## # A tsibble: 232 x 9 [1Y] ## # Key: Country [4] ## Country Code Year GDP Growth CPI Imports Exports Population ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark DNK 1960 6248946880. NA 8.25 34.3 32.3 4579603 ## 2 Denmark DNK 1961 6933842099. 6.38 8.53 32.3 30.0 4611687 ## 3 Denmark DNK 1962 7812968114. 5.67 9.16 32.5 28.6 4647727 ## 4 Denmark DNK 1963 8316692386. 0.637 9.72 30.8 30.4 4684483 ## 5 Denmark DNK 1964 9506678763. 9.27 10.0 32.6 29.9 4722072 ## 6 Denmark DNK 1965 10678897387. 4.56 10.6 31.5 29.3 4759012 ## 7 Denmark DNK 1966 11721248101. 2.74 11.3 30.8 28.6 4797381 ## 8 Denmark DNK 1967 12788479692. 3.42 12.2 30.0 27.3 4835354 ## 9 Denmark DNK 1968 13196541952 3.97 13.2 29.7 27.7 4864883 ## 10 Denmark DNK 1969 15009384585. 6.32 13.7 30.4 27.6 4891860 ## # … with 222 more rows nordic_economy %&gt;% autoplot(GDP) fitnord &lt;- nordic_economy %&gt;% model( trend_model = TSLM(GDP ~ trend()), trend_model_ln = TSLM(log(GDP) ~ trend()), ets = ETS(GDP ~ trend(&quot;A&quot;)), arima = ARIMA(GDP) ) fitnord ## # A mable: 4 x 5 ## # Key: Country [4] ## Country trend_model trend_model_ln ets arima ## &lt;fct&gt; &lt;model&gt; &lt;model&gt; &lt;model&gt; &lt;model&gt; ## 1 Denmark &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(1,1,1)&gt; ## 2 Finland &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(0,1,2)&gt; ## 3 Norway &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(0,1,1)&gt; ## 4 Sweden &lt;TSLM&gt; &lt;TSLM&gt; &lt;ETS(M,A,N)&gt; &lt;ARIMA(0,1,2)&gt; Denmark: ARMA(1,1) Finland: MA(2) Norway: MA(1) Sweden: MA(2) fitnord %&gt;% coef() ## # A tibble: 39 × 7 ## Country .model term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark trend_model (Intercept) -5.65e+10 8.75e+9 -6.46 2.70e- 8 ## 2 Denmark trend_model trend() 6.63e+ 9 2.58e+8 25.7 1.14e-32 ## 3 Denmark trend_model_ln (Intercept) 2.30e+ 1 8.55e-2 269. 7.68e-89 ## 4 Denmark trend_model_ln trend() 7.12e- 2 2.52e-3 28.3 7.68e-35 ## 5 Denmark ets alpha 1.00e+ 0 NA NA NA ## 6 Denmark ets beta 3.67e- 1 NA NA NA ## 7 Denmark ets l[0] 4.92e+ 9 NA NA NA ## 8 Denmark ets b[0] 1.24e+ 9 NA NA NA ## 9 Denmark arima ar1 -3.90e- 1 2.06e-1 -1.89 6.36e- 2 ## 10 Denmark arima ma1 7.24e- 1 1.43e-1 5.05 4.84e- 6 ## # … with 29 more rows fitnord %&gt;% glance() ## # A tibble: 16 × 21 ## Country .model r_squared adj_r_squared sigma2 statistic p_value df ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Denmark trend_mod… 0.922 0.920 1.08e+21 660. 1.14e-32 2 ## 2 Denmark trend_mod… 0.935 0.933 1.03e- 1 800. 7.68e-35 2 ## 3 Denmark ets NA NA 1.04e- 2 NA NA NA ## 4 Denmark arima NA NA 2.41e+20 NA NA NA ## 5 Finland trend_mod… 0.914 0.912 7.34e+20 594. 1.70e-31 2 ## 6 Finland trend_mod… 0.930 0.929 1.14e- 1 745. 4.96e-34 2 ## 7 Finland ets NA NA 1.32e- 2 NA NA NA ## 8 Finland arima NA NA 1.89e+20 NA NA NA ## 9 Norway trend_mod… 0.824 0.821 4.60e+21 262. 8.54e-23 2 ## 10 Norway trend_mod… 0.959 0.958 8.37e- 2 1307. 1.64e-40 2 ## 11 Norway ets NA NA 8.23e- 3 NA NA NA ## 12 Norway arima NA NA 6.78e+20 NA NA NA ## 13 Sweden trend_mod… 0.919 0.918 2.65e+21 635. 3.07e-32 2 ## 14 Sweden trend_mod… 0.935 0.933 8.19e- 2 800. 7.57e-35 2 ## 15 Sweden ets NA NA 1.16e- 2 NA NA NA ## 16 Sweden arima NA NA 8.84e+20 NA NA NA ## # … with 13 more variables: log_lik &lt;dbl&gt;, AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, ## # CV &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt;, MSE &lt;dbl&gt;, ## # AMSE &lt;dbl&gt;, MAE &lt;dbl&gt;, ar_roots &lt;list&gt;, ma_roots &lt;list&gt; fitnord %&gt;% accuracy() %&gt;% arrange(Country, MPE) ## # A tibble: 16 × 11 ## Country .model .type ME RMSE MAE MPE MAPE MASE RMSSE ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Denmark trend_mod… Trai… -1.12e+10 6.89e10 3.67e10 -5.17 28.0 3.34 4.24 ## 2 Denmark ets Trai… 4.50e+ 7 1.65e10 1.04e10 0.518 7.09 0.946 1.02 ## 3 Denmark arima Trai… 4.40e+ 9 1.51e10 1.04e10 5.05 8.16 0.945 0.930 ## 4 Denmark trend_mod… Trai… -2.06e- 6 3.23e10 2.63e10 51.1 80.8 2.40 1.99 ## 5 Finland trend_mod… Trai… -8.61e+ 9 5.64e10 2.99e10 -5.53 28.6 2.95 3.82 ## 6 Finland ets Trai… 1.36e+ 8 1.47e10 9.41e 9 0.795 8.36 0.927 0.996 ## 7 Finland arima Trai… 3.54e+ 9 1.34e10 9.14e 9 5.03 8.92 0.900 0.906 ## 8 Finland trend_mod… Trai… 9.54e- 7 2.66e10 2.21e10 46.1 80.5 2.18 1.80 ## 9 Norway trend_mod… Trai… -1.31e+10 8.20e10 3.51e10 -4.24 24.9 2.24 3.01 ## 10 Norway ets Trai… -5.29e+ 8 2.75e10 1.37e10 0.755 6.94 0.870 1.01 ## 11 Norway arima Trai… 4.90e+ 9 2.56e10 1.40e10 5.04 8.11 0.890 0.938 ## 12 Norway trend_mod… Trai… 2.09e- 6 6.67e10 5.48e10 130. 181. 3.49 2.45 ## 13 Sweden trend_mod… Trai… -1.18e+10 8.23e10 4.79e10 -3.96 23.7 2.25 2.68 ## 14 Sweden ets Trai… 1.19e+ 9 3.02e10 1.86e10 0.745 7.64 0.875 0.984 ## 15 Sweden arima Trai… 8.48e+ 9 2.89e10 2.01e10 5.18 9.37 0.942 0.944 ## 16 Sweden trend_mod… Trai… 2.10e- 6 5.05e10 3.90e10 29.4 53.3 1.83 1.65 ## # … with 1 more variable: ACF1 &lt;dbl&gt; "],["producing-forecasts-1.html", "Chapter 23 Producing forecasts 23.1 Example: Gross Domestic Product data", " Chapter 23 Producing forecasts 23.1 Example: Gross Domestic Product data library(tsibbledata) # Data sets package global_economy %&gt;% model(trend_model = TSLM(GDP ~ trend())) -&gt; fit fit %&gt;% forecast(h = &quot;3 years&quot;) -&gt; fcast3yrs fcast3yrs ## # A fable: 789 x 5 [1Y] ## # Key: Country, .model [263] ## Country .model Year GDP .mean ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dist&gt; &lt;dbl&gt; ## 1 Afghanistan trend_model 2018 N(1.6e+10, 1.3e+19) 16205101654. ## 2 Afghanistan trend_model 2019 N(1.7e+10, 1.3e+19) 16511878141. ## 3 Afghanistan trend_model 2020 N(1.7e+10, 1.3e+19) 16818654627. ## 4 Albania trend_model 2018 N(1.4e+10, 3.9e+18) 13733734164. ## 5 Albania trend_model 2019 N(1.4e+10, 3.9e+18) 14166852711. ## 6 Albania trend_model 2020 N(1.5e+10, 3.9e+18) 14599971258. ## 7 Algeria trend_model 2018 N(1.6e+11, 9.4e+20) 157895153441. ## 8 Algeria trend_model 2019 N(1.6e+11, 9.4e+20) 161100952126. ## 9 Algeria trend_model 2020 N(1.6e+11, 9.4e+20) 164306750811. ## 10 American Samoa trend_model 2018 N(6.8e+08, 1.7e+15) 682475000 ## # … with 779 more rows fcast3yrs %&gt;% filter(Country == &quot;Sweden&quot;, Year == 2020) %&gt;% str() ## fbl_ts [1 × 5] (S3: fbl_ts/tbl_ts/tbl_df/tbl/data.frame) ## $ Country: Factor w/ 263 levels &quot;Afghanistan&quot;,..: 232 ## $ .model : chr &quot;trend_model&quot; ## $ Year : num 2020 ## $ GDP : dist [1:1] ## ..$ 3:List of 2 ## .. ..$ mu : num 5.45e+11 ## .. ..$ sigma: num 5.34e+10 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;dist_normal&quot; &quot;dist_default&quot; ## ..@ vars: chr &quot;GDP&quot; ## $ .mean : num 5.45e+11 ## - attr(*, &quot;key&quot;)= tibble [1 × 3] (S3: tbl_df/tbl/data.frame) ## ..$ Country: Factor w/ 263 levels &quot;Afghanistan&quot;,..: 232 ## ..$ .model : chr &quot;trend_model&quot; ## ..$ .rows : list&lt;int&gt; [1:1] ## .. ..$ : int 1 ## .. ..@ ptype: int(0) ## ..- attr(*, &quot;.drop&quot;)= logi TRUE ## - attr(*, &quot;index&quot;)= chr &quot;Year&quot; ## ..- attr(*, &quot;ordered&quot;)= logi TRUE ## - attr(*, &quot;index2&quot;)= chr &quot;Year&quot; ## - attr(*, &quot;interval&quot;)= interval [1:1] 1Y ## ..@ .regular: logi TRUE ## - attr(*, &quot;response&quot;)= chr &quot;GDP&quot; ## - attr(*, &quot;dist&quot;)= chr &quot;GDP&quot; ## - attr(*, &quot;model_cn&quot;)= chr &quot;.model&quot; fcast3yrs %&gt;% filter(Country==&quot;Sweden&quot;) %&gt;% autoplot(global_economy) + ggtitle(&quot;GDP for Sweden&quot;) + ylab(&quot;$US billions&quot;) "],["communicating-forecast-uncertainty.html", "Chapter 24 Communicating forecast uncertainty 24.1 The Red River Flood of 1997", " Chapter 24 Communicating forecast uncertainty 24.1 The Red River Flood of 1997 "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
