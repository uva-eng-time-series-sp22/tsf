--- 
title: "Time Series and Forecasting:\n A Project-based Approach with R"
author: "Arthur Small"
date: "Version of: `r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    config:
      toc:
        collapse: section
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Class notes for University of Virginia SYS 5581, Spring 2021"
---

# {-}

This document is a compilation of class notes for SYS 5581 *Time Series and Forecasting*, University of Virginia, Spring, 2021.


```{r load basic packages, include=FALSE}
library(tidyverse)
library(fpp3)
```

```{r setup, include=FALSE, cache=F, message=F, warning=F, results="show"}
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(fig.path='graphics/')
knitr::opts_chunk$set(cache.path='_cache/')
knitr::opts_chunk$set(warning=F, message=F)
# knitr::opts_knit$get(kable.force.latex = TRUE)
```

<!--chapter:end:index.Rmd-->

# Preface {-}

This document contains class notes and other materials related to SYS 5581 *Time Series and Forecasting* at the University of Virginia.


## Readings and references {-}

### Time series {-}

FPP3 = Hyndman, R.J., & Athanasopoulos, G. [-@robjhyndmanForecastingPrinciplesPractice2021] [*Forecasting: principles and practice, 3rd edition*](https://otexts.com/fpp3/), OTexts: Melbourne, Australia. OTexts.com/fpp3

TFS = [these notes]

### Statistics with R {-}

### Data science with R, general {-}

R4DS = Wickham, Hadley, and Garrett Grolemund, [*R for Data Science*](https://r4ds.had.co.nz/)

TSDS = Carrie Wright, Shannon E. Ellis, Stephanie C. Hicks and Roger D. Peng, [*Tidyverse Skills for Data Science*](https://jhudatascience.org/tidyversecourse/)

Tibshirani, Ryan, [Statistics 36-350 *Statistical Computing*](https://www.stat.cmu.edu/~ryantibs/statcomp/), Carnegie-Mellon University, Fall 2019

### Other references on Zotero {-}

A variety of other references to resources on time series and forecasting are gathered in [the Zotero library for this course](https://www.zotero.org/groups/2555186/uva_engineering_time_series_and_forecasting/library).


## Acknowledgements  {-}

These notes are organized using the **bookdown** package [@R-bookdown], which was built on top of R Markdown and **knitr** [@xie2015].

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:00-preface.Rmd-->

# Course Syllabus {-}

title:  "SYS 5581 Time Series and Forecasting"
author: "Instructor: Arthur Small"
date:   "University of Virginia Engineering, Spring 2021"

*Class meetings:* MW 09:30-10:45 a.m. online via Zoom

*Office Hours:* MW 11:00 a.m.-12:30 p.m. online via Zoom (subject to change). Sign up in advance for a 45-minute session via the Collab "Sign Up" tool. If you cannot make any scheduled time, please contact the instructor via email to schedule an appointment. Meetings online via Zoom: <https://virginia.zoom.us/my/arthursmalliii>

*Web Resources:*

  * [Collab class site](https://collab.its.virginia.edu), for basic course information, assignments, office hours sign-up, links to online textbook and other resources.
  * [Github class site](https://github.com/uva-eng-time-series-sp21), for posting and sharing code.
  * Zoom, for class sessions, recordings, and office hours.

## Course Description {-}

The course is designed to introduce graduate students and advanced undergraduates in engineering to time series and forecasting. The course will not include a deep exploration of theory. Rather, the goal is for students by the end to be able to analyze time series data competently, as part of their work designing and working with engineered systems. 

In addition to learning theory, each student will undertake a semester-long research project. Ideally, this project will relate closely to the student’s own dissertation research, professional practice, or other domain application that interests them. My hope is that these projects could form the basis for subsequent research papers, dissertation chapters, or other professional work products, for interested students.

The course will, therefore, be structured primarily as a *workshop*: the ultimate goal is to help you to create a professionally presented report. Our workflow will, therefore, be subject to revision, according to my judgement of how best to use our time to help you produce a professional report.

The course outline is divided into two major sections. First, we will introduce the theory, with examples. In the later part of the semester, we will focus on workshopping your projects in progress.

Important: class readings are subject to change, contingent on mitigating circumstances and the progress we make as a class.  Students are encouraged to attend lectures and check the course website for updates. 


### Prerequisites {-}

Students should have taken at least one rigorous intermediate course in probability and statistics. They should be comfortable with the representation of uncertain information in the form of probability distributions, with conditional probabilities, and with other such foundational concepts. 

In addition, to carry out the data analyis, the student should have at least ability be able to code, in some general-purpose language. For this course, we will work in R, focusing on specialized packages for working with time series data and generating forecasts.

### Expectations {-}

Each student will make a presentation on their data analysis project. Students will be evaluated based on their performance in these presentations and on their final project, on occasional short quizzes; and on their contributions inside and outside of class towards helping other students. 

### Readings  {-}

The primary text for the course will be [*Forecasting: Principles and Practice*, 3rd ed.](https://otexts.com/fpp3/) by
Rob J. Hyndman and George Athanasopoulos. This resource is available for free online and is linked from the Collab site. The text includes example code in R, and covers several useful R packages related to time series and forecasting.

Additional readings including relevant articles will be provided as the course progresses. The choice of readings will depend in part on student interests, as conveyed through their choice of projects. 

### Course Objectives {-}

1. Students will learn the foundations of time series and forecasting.

2. Students will gain the experience of building statistical models of time series, and models for forecasting, and will learn how to evaluate their performance.

3. Students will learn the concepts and practice of *reproducible research*, in the course of preparing a research paper.

4. Students will gain experience in making presentations and in preparing a polished research article.


### Grading Policy {-}

- **10%** of your grade will be determined by quizzes designed to test your understanding of the theoretical concepts introduced in class. This quiz will delivered at roughly the mid-point of the semester. It will be open-book and open-notes, outside of class. You will have multiple days to complete it. The quiz will not be designed to be especially challenging: the goal is to give you the opportunity to synthesize your understanding of core concepts, in preparation for developing your data analysis for your research project.

- **10%** of your grade will be determined by your performance in one in-class presentation based on your project. These presentations will be scheduled when you are, in the judgement of the instructor, far enough along to do so.

- **10%** of your grade will be determined by your contributions to assist other students. These contributions can come through class participation, by making useful contributions in online forums (Github), or through other means that add value to the group experience.

- **70%** of your grade will be determined by your performance on your final project. The development of the project will include multiple iterations, each with an associated deliverable: 

  * An initial Concept Note.
  * A more developed Project Proposal.
  * A first working draft of your final project paper.
  * A final complete draft of your project paper.

Details of these staged intermediate deliverables will be forthcoming. The final product should be a polished professional paper that meets academic standards regarding format, quality, and integrity.


### Attendance Policy {-}

Regular attendance is very much in your pedagogic interest. However, it is up to you whether to attend in person or to view recorded class sessions afterwards.

### Communications protocols, including emails and office hours {-}

I prefer to avoid using email to communicate with students about class matters. For substantive questions about course materials and concepts, please use class time, office hours, or meetings by appointment. Please use email only for brief clarifying questions, or to set up appointments. 

### Academic Dishonesty Policy {-}

Don’t cheat. Don't plagiarize. Don't present someone else's work as your own.

### Disabilities Policy {-}

Together with the University of Virginia, I am committed to assuring that all students have the full opportunity to benefit from the course regardless of their disability status. If you have a disability that may require accommodations, please see the instructor early in the semester to work out appropriate arrangements.




<!--chapter:end:01-syllabus.Rmd-->

# (PART) Introduction and Overview {-} 

# Introduction {#intro}

Readings: FPP3, Ch. 1

## What is time series analysis?

## Time series data

## Time series patterns

Examples.

## Types of problems that are amenable to time series analysis

## Time series and forecasting

## Overview of the course








<!--chapter:end:02-intro.Rmd-->

# Project workflow

Readings: 

 * [FPP3, Section 5.1](https://otexts.com/fpp3/a-tidy-forecasting-workflow.html)
 * [TSDS, Sections 5.1-5.3](https://jhudatascience.org/tidyversecourse/model.html#about-this-course-4)

Case study examples:

 * [Open Case Studies: Exploring CO2 emissions across time](https://www.opencasestudies.org/ocs-bp-co2-emissions/)
 * [Open Case Studies: Predicting Annual Air Pollution](https://www.opencasestudies.org/ocs-bp-air-pollution/) 

Steps in a time series statistical analysis:

1. State your question
2. Acquire data and background information
3. Organize your data
4. Perform exploratory analysis of your data
5. Write down your model of the data generating process
6. If necessary: transform the model and data to make it ready for analysis
7. Choose an appropriate technique for estimating model parameters, consistent with your assumptions about your data generating process
8. Estimate model parameters
9. Confirm that your modeling assumptions are satisfied; 
10. Compute measures of model quality; confirm that your model is good enough for your purpose
11. Use your calibrated model to address your original question. 

Examples of using your model:

  - Generate a forecast of future events
  - Estimate the probability of a future event
  - $\ldots$


## Example problem: Estimating the probability of a weather event {-}

A pub owner in Charlottesville plans to sell beer outside on St. Patrick's Day, March 17. The pub owner must decide whether to arrange to rent a supplemental refrigeration system for the day. 

Supplemental refrigeration offers a form of insurance. If temperatures outside on March 17 are high and the pub owner has not arranged supplemental refrigeration, she will be left with warm beer that she will have difficulty selling, leading to financial losses. Conversely, if she pays for supplemental refrigeration when temperatures are low, she will have incurred an unnecessary expense.

To decide whether to insure herself against loss, she wishes to estimate the probability distribution of temperatures on March 17. 

## State your question

**1. What is the probability that the high temperature on March 17 will exceed 23 degrees Celsius?**

## Acquire data and background information

Get historical temperature data. 

(Here, will simulate the data.)

```{r simulate historical data - no covariates, message=FALSE}
set.seed(1)       # Keeps random data from changing each time the code is run

theta <-  18      # True long-run average temp
sigma  <-  3      # True stnd dev of temp around this mean
n      <- 40      # n = number of simulated historical data points
                     # We don't use 'T' because in R language 'T' = logical 'TRUE'

y <- rnorm(n,theta,sigma)
```

## Organize your data

Let $y_1, \ldots, y_T$ denote the high temperature in Charlottesville on March 17 for each of the previous $T$ years.

```{r}
print(y)
```

## Perform exploratory analysis of your data

```{r }
hist(y, breaks = 20, main = "Histogram of historical temperatures in degrees C")
```

```{r}
boxplot(y)
```


## Write down your model of the data generating process {#white-noise-model}

It is supposed that these data were generated as independent, identically distributed random draws from a normal distribution: for $t = 1, \ldots, T$,

$$y_t = \theta + \varepsilon_t$$
where $\theta$ denotes the true but unobserved value of the long-run average temperature, and where $\varepsilon_t \sim N(0, \sigma^2)$. 

### Comments on this statistical model: The risk of model mis-specification

This model asserts several substantive assumptions about the data generating process.

  * The process is assumed to be *stationary*. There is no upward trend over time, no long-term climate change, etc.
  
  * Temperatures are assumed to be *independent* from one year to the next. In particular, there is no *autocorrelation*. Knowing that one year's temperature was unusually high (say) provides no information about the likelihood that next year's temperature will also be unusually high. Inter-annual climate cycles (e.g., due to $El\ Ni\tilde{n}o$) are ruled out.
  
  * Temperature variations around the long-run average are assumed to be *identically distributed*. This assumption rules out the possibility that variance is, say, greater when temperatures are higher than when they are lower.
  
And others.
  
In general, it is important to formulate a statistical model that accurately reflects the true characteristics of the underlying data generating process.

When your statistical model is mis-specified, your probabilistic forecast of future events are likely to stray from the true underlying probabilities. Model mis-specification can then lead to inaccurate estimates of the distribution of losses for each possible action. This error may in turn lead to selection of a sub-optimal action.

A particular problem to guard against is the possibility to underestimate the likelihood of extreme events that could cause catastrophic losses. 

That said, your time on this Earth is limited. Depending on the decision problem and the stakes involved, refining your model to get sharper loss estimates may or may not be worth the bother.

A reasonable approach is to start by first writing down a simple forecasting model that appears to capture the essence of the process as you understand it. On the basis of this simple model, generate first-cut probabilistic forecasts of uncertain events. Use these to generate estimated distributions of losses for each possible action in your action set. On that basis, use the specified decision criterion to derive an initial optimal decision rule.

Then, go back and check things over more carefully. Review the realism of your statistical model, given your understanding of data generating process. Plot and examine the distribution of your prediction errors. Do your prediction errors appear to follow a pattern that matches what you would expect, given the assumptions you have made? 

If you find evidence that your prediction model is mis-specified, it may be worth it to go back and refine your model, and see if you generate different results.

One very good idea is to perform a *sensitivity analysis*. How sensitive are your decision recommendations and outcomes to the assumptions you've built into your statistical model? If you are not highly confident in your statistical assumptions, and if those assumptions turn out to matter a lot for your recommendations and outcomes, then it could very well be worth the bother to revisit those assumptions, and investigate alternatives.

On the other hand, if your decision recommendations are not highly sensitive to your statistical assumptions, then keeping your initial model may be defensible. The point of this work is *not* to build the best possible prediction system, bullet-proof against any statistical criticism. The point is to help people make good decisions -- or at least, decisiions better than they would have made otherwise. Your time and other resources are limited. A good-enough model may be good enough.


## If necessary: transform the model and data to make it ready for analysis

Not needed here.

Will consider many cases where it is.


## Choose an appropriate technique for estimating model parameters, consistent with your assumptions about your data generating process

For this case, ordinary least squares (OLS) estimation is just fine.

## Estimate model parameters


```{r estimate model parameters using ordinary least squares}
theta_hat <- mean(y)         # Sample mean

epsilon_hat <- y-theta_hat   # Model residuals

ssr <- sum(epsilon_hat^2)    # Sum of squared residuals

sigma_hat <- ssr/(n-1)       # Estimated standard error

print(theta_hat)

print(sigma_hat)
```


## Confirm that your modeling assumptions are satisfied


## Compute measures of model quality; confirm that your model is good enough for your purpose


```{r}
forecast_bias <- -0.5
forecast_std_error <- 1

forecast_errors <- rnorm(n,mean = forecast_bias,sd = forecast_std_error)

x <- y + forecast_errors

linear_model <- lm(y~x)

plot(x,y, xlab = "Forecast temperature", ylab = "Observed temperature")
abline(linear_model)
```


```{r}
plot(x,y)
```




## Use your calibrated model to address your original question

```{r generate distribution of temp based on model results}
sim_y <- rnorm(10000, theta_hat, sigma_hat)

hist(sim_y, breaks = 100)
```




<!-- ## For next time: Add a covariate -->

<!-- Simulation of the prediction process, one covariate -->



```{r bivariate normal, include=F, eval= FALSE, message=FALSE}
# library(MASS)


Sigma <- matrix(c(10,3,3,2),2,2)
Sigma
var(mvrnorm(n=1000, rep(0, 2), Sigma))
var(mvrnorm(n=1000, rep(0, 2), Sigma, empirical = TRUE))
```

```{r generate bivariate normal, include=F, eval=FALSE, message=FALSE}
# library(MASS)     # Load packages for working with multivariate normal distributions

set.seed(1)       # Keep random data from changing each time the code is run

# Variables
# y : temperature on March 17
# x : covariate, e.g., forecast of temp on Mar 17

theta <- 18

Theta <-  c(0,18)      # True long-run average
Sigma  <-  3      # True stnd dev of temp around this mean
N      <- 40      # number of simulated historical data points


x <- rnorm(N, mean=theta, sd=sigma_x)


y <- rnorm(n,theta,sigma)

hist(y, breaks = 10, main = "Histogram of historical temperatures in degrees C")

```


<!--chapter:end:03-project-overview.Rmd-->

# Assignment: Write a concept note

Write a concept note for a potential time series analysis project. 

## Assignment Instructions

In just a few paragraphs (1 page max), describe an application of time series analysis that you might undertake in connection with your own research, your professional practice, or for a class project.

## Submission procedure

Prepare your note in R Markdown within R Studio. Use `knitr` to generate a PDF output.

Commit your saved .Rmd and .pdf files to your (local) repo, then push your changes to your repo on the course site on Github.

When all is ready, submit your assignment on Collab. But don't attach your pdf file. Instead, a link to your .pdf file on Github. 

Generate your output as a PDF rather than as HTML: Github makes is complicated to download and view .html files.
 

## Choosing a topic

Your topic might depend on which type of degree program you are in, and how far along you are in that program.

If you are well along in a research-oriented graduate degree program, then you likely have already a fairly clear notion of the research question(s) you are addressing or plan to address, and the methods you will use to approach your question. Ideally, you will have already secured access to a time series data set that you hope to analyze. In this case, please describe the data set, and the type of useful information you hope to learn about it. If applicable, describe how this analysis could inform your research program.

If you at the beginning stages of a research-oriented degree program, you likely have some idea of the topics and methods you will use, but may not have much idea of what data sets you will use or what insights you hope to extract from those data. In this case, please think about what kinds of time series data analysis might help advance your work. It is likely that a consultation with your research advisor will be helpful. Write about your research question(s), and how analysis of time series data might help your research program. Ideally, please try to identify a data set that you could use, and describe how you can access these data.

If you are not in a research-oriented degree program, think about how a time series analysis could be applied to some aspect of your work, professional practice, or personal or career interests.

If you don't have any ideas at all, and are looking for help finding one, let me know. We will identify together a project idea and data set you can work with, possibly related to energy. But try first to think of one on your own, one that you care about.


<!--chapter:end:05-assignment-concept-note.Rmd-->

# (PART) Set up {-} 

# Project set up: Good practices

## Reproducible workflows

## Setting up a new project

## Folder structure

Readings:

[TSDS Section 1.6](https://jhudatascience.org/tidyversecourse/intro.html#data-science-project-organization)

## Naming things

Readings: 

Jenny Bryan, [naming things](https://speakerdeck.com/jennybc/how-to-name-files) slide deck, Reproducible Science Workshop, 2015.

Hadley Wickham, [The tidyverse style guide. Section 1: "Files"](https://style.tidyverse.org/files.html#names)


<!--chapter:end:11-project_setup.Rmd-->

# Assignment: Set up your computing environment

The course relies on computing resources. Please install the software as indicated on your local machine, and familiarize yourself with the associated documentation. 

Topics: R, R Studio, git, Github, Markdown, R Markdown, Tidyverse and tidyverts packages for R

Assignment: Follow instructions in the course [Computing setup guide](https://github.com/uva-eng-time-series-sp21/sys5581-course-materials/blob/master/computing_setup_guide.pdf).



## The R programming language, and related resources

We will do our coding in R, a programming language especially well-suited to statistical computing. 

* [Download and install R](https://cran.rstudio.com/), v. 3.0.1+.
  - Note: There is a later version, v. 4.0.2, in development, but you shouldn't need it.

[R Studio](https://rstudio.com/products/rstudio/) is an integrated development environment (IDE) for R. It offers a variety of utilities to enhance the experience of coding and generating documents.

* [Download and install R Studio](https://rstudio.com/products/rstudio/download/#download), v. 1.4.1+. 

[Tidyverse](https://www.tidyverse.org/) is a collection of packages that extend the capabilities of R for doing data science.

* Install the Tidyverse packages for R: From the Console tab in R Studio (or from R running in a Terminal window), enter:
```{r, eval=FALSE}
install.packages("tidyverse")
```
  - Alternatively, you may install packages via the `Packages` tab in R Studio.

* Optional: To learn how to wrangle and visualize data using the Tidyverse packages, you may find it useful to go through the [Tidyverse Fundamentals with R](https://learn.datacamp.com/skill-tracks/tidyverse-fundamentals) modules on [Datacamp](https://learn.datacamp.com/). 
  - Datacamp also offers [a range of other learning modules for developing data science skills in R](https://learn.datacamp.com/career-tracks/data-scientist-with-r). 

[Tidyverts](https://tidyverts.org/) is a collection of R packages for time series analysis designed to work well with the Tidyverse packages. Each package in the tidyverts suite needs to be installed individually:

* From the Console tab in R Studio (or from R running in a Terminal window), enter:
```{r, eval=FALSE}
install.packages(c("tsibble", "tsibbledata", "feasts", "fable"))
```

  - You don't need to install the `tsibbletalk` and `fable.prophet` packages; we probably won't use them in this course. 
  



## Git and Github

Reference: [Happy Git and GitHub for the useR](https://happygitwithr.com/)

[Git](https://git-scm.com/) is software for version control. Github is a web service that provides remote storage and access to files via git. This setup greatly facilitates collaboration between multiple individuals working on the same code base.

First watch this short YouTube video to get an orientation to git and Github: [Git and GitHub for an Organized Project (STAT 545 Episode 2-A)](https://www.youtube.com/watch?v=l2ftm-YwJs8) from the University of British Columbia.

Then install git on your machine and link it to your R Studio instance and your file repository on Github:

* [Follow these instructions](https://jennybc.github.io/2014-05-12-ubc/ubc-r/session03_git.html) to [download and install git](https://git-scm.com/downloads) and to link git with R Studio.

A collection of files associated with a single project is in git-speak called a "repository" or *"repo"*. You should already have a basic repo set up for you on the course site on Github. The next step is to copy ("clone") this remote repo to your local machine.

* Clone your course repo on Github to a new R Studio project on your local machine.
  - Navigate to [the course website on Github](https://github.com/uva-eng-time-series-sp21). Select your repo.
  - Click on the green button labeled "Code". Copy the URL.
  - In the R Studio window, from the pull-down menu in the upper-right corner, select `New Project...`, `Version Control`, `Git`. Paste the URL into the dialog box labeled `Repository URL`.
  - Optional: Change the name of the project folder, and the location of this folder on your local directory tree.
  - Click on `Create Project`. The files from your remote repo should be copied to your local machine in a new folder with the name you chose.
  
* Optional: [Download and install the Github desktop client](https://git-scm.com/downloads/guis), or an alternative GUI client.
  - The git operations you need for this course can be managed within R Studio, from the `Git` tab. Some more advanced operations require using either a Terminal window, or a Git desktop client. 
  
As you get going, you will likely want to learn more about how to work with git and Github. [Review the documentation for git](https://git-scm.com/) and [this Github Guide](https://guides.github.com/introduction/flow/). Learn the basics.


### Using personal tokens to access Github

Github is phasing out the use of passwords for authorizations. 
```
---- Forwarded Message -----
From: GitHub <noreply@github.com>
To: Arthur Small <asmall@virginia.edu>
Sent: Sunday, February 21, 2021, 6:20:58 AM EST
Subject: [GitHub] Deprecation Notice

Hi @arthursmalliii,

You recently used a password to access the repository at uva-eng-time-series-sp21/coronato-nicholas with git using git/2.30.0.

Basic authentication using a password to Git is deprecated and will soon no longer work. Visit https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information around suggested workarounds and removal dates.

Thanks,
The GitHub Team
```

Instead, you must create a personal access token. See the Github documentation.

## Markdown and R Markdown

Markdown is a markup language: a set of formatting instructions for rendering documents. R Markdown is an extension of Markdown that allows for embedding chunks of R code into a Markdown document. In this course, we will write our work in R Markdown within the R Studio environment, then use the `knitr` package to generate HTML and PDF output files.

For a nice introduction to Markdown and R Markdown, watch the short YouTube video [Reproducible Reports with R Markdown (STAT 545 Episode 3-A)](https://www.youtube.com/watch?v=ZzDSkBgt9xQ) from the University of British Columbia.

As you proceed in creating your documents, you will probably want to access additional resources:

* From within R Studio, you can access an R Markdown Cheat Sheet via `Help/Cheatsheets`.

* Markdown reference: https://www.markdownguide.org/

* R Markdown reference: https://rmarkdown.rstudio.com/


## Bibliographic resources: Zotero and Bibtex

[Coming soon...]

## General course web resources

  * [Collab class site](https://collab.its.virginia.edu), for basic course information, assignments, office hours sign-up, links to online textbook and other resources.
  * [Github class site](https://github.com/uva-eng-time-series-sp21), for posting and sharing code.
  * Zoom, for class sessions, recordings, and office hours.


<!--chapter:end:12-assignment-computing_setup.Rmd-->

# (PART) Data Acquisition and Preparation {-} 

# Finding a dataset for your project

```{r, out.width='100%', fig.align='center', fig.cap='New York City 311 calls by time of day, September 8--15, 2010. Source: [Wired Magazine](https://www.wired.com/2010/11/ff_311_new_york/)', echo=FALSE}
knitr::include_graphics('graphics/ff_311_newyork1b_f.jpg')
```

Readings:

[TSDS, Section 5.4](https://jhudatascience.org/tidyversecourse/model.html#data-needs)

## Appropriate data sets for time series analysis

Just because your data set has some dates and times in it, doesn't mean it is appropriate for a time series analysis.

Time series methods are generally appropriate when data are measured at regular intervals, over a fairly long period.

## Data resources

The UVA Libraries offer excellent [data services](https://www.library.virginia.edu/services), including resources for [data discovery and access](https://data.library.virginia.edu/datasources/). If you haven't settled on your own dataset to analyze for your project, you may find one by browsing their [recommended top data sources](https://data.library.virginia.edu/datasources/find-data/) and [licensed data](https://data.library.virginia.edu/datasources/licensed/). If you need personal assistance, you are invited to contact UVA's Data Librarian, Jenn Huck, at data@virginia.edu to schedule an appointment. 

Some data sources you might check out:

* The [Cross-National Time-Series Data Archive](https://data.library.virginia.edu/datasources/licensed/cnts/) provides more than 200 years of annual data from 1815 onward for over 200 countries. It consists of 196 data variables used by academia, government, finance and media.
* [U.S. Energy Information Administration](https://www.eia.gov/tools/). Diverse datasets on energy.
* "[Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)".
* [Flowing Data](https://flowingdata.com/category/statistics/data-sources/) data sources.
* [The Stanford Open Policing Project](https://openpolicing.stanford.edu/). "On a typical day in the United States, police officers make more than 50,000 traffic stops. Our team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country."
* [Open U.S. Federal Government Data](https://www.data.gov/opendata/).
* [Charlottesville, Virginia open data](https://opendata.charlottesville.org/)
* [New York City 311 Data](https://portal.311.nyc.gov/article/?kanumber=KA-02893).






<!--chapter:end:17-find-a-dataset.Rmd-->

# Data acquisition and extraction

Readings:

[TSDS, Chapter 2](https://jhudatascience.org/tidyversecourse/get-data.html)

## Access protocols and permissions

* Reproducible extraction of data from source location: may be complicated by access protocols.

  - access tokens; APIs
  - raw data from github for private repos
  - databases
  - package `httr` to access data from websites


## Accessing databases

```{r open connection to database 2, eval=FALSE, include=FALSE}
# Open connection to a remote database
# Make sure your VPN network connection is active if needed!

# if(!('RPostgreSQL' %in% installed.packages())) install.packages('RPostgreSQL')
library(RPostgreSQL)

# file "my_postgres_credentials.R" contains the log-in information
# DON'T EVER HARD CODE ACCESS CREDENTIALS INTO YOUR SOURCE CODE!!!
source("my_postgres_db_credentials.R")


# Open connection
db_driver <- dbDriver("PostgreSQL")
db <- dbConnect(db_driver,user=user, password=password,dbname="postgres", host=host)
rm(password) 

# check the connection: If function returns value TRUE, the connection is working
dbExistsTable(db, "metadata")
```

```{r retrieve data from db 2, eval=FALSE, message=FALSE}
esales <- dbGetQuery(db,'SELECT * from eia_elec_sales_va_all_m') # SQL code to retrieve data from a table in the remote database
# str(esales)
esales <- as_tibble(esales) # Convert dataframe to a 'tibble' for tidyverse work
# str(esales)
```

```{r save data in Apache Arrow format 2, eval=FALSE}
# Reference: https://arrow.apache.org/docs/r/
# if(!('arrow' %in% installed.packages())) install.packages('arrow')
library(arrow)
write_feather(esales, "esales.feather")
```

```{r close db connection 2, eval=FALSE}
# Close connection -- this is good practice
dbDisconnect(db)
dbUnloadDriver(db_driver)
```


## Other comments

Make your extraction code "as reproducible as possible", subject to these access constraints. At minimum, document clearly how you obtained the data, so others could follow your path, even if not via pure code.
 
Keep your raw data in read-only mode. Don't edit these files. 

Write code to transform the raw data into form you will use for analysis. Don't do it manually.



<!--chapter:end:18-extract-data.Rmd-->

# Assignment: Update your concept and get your data



## Refine your concept note

  - State clearly the question(s) you will attempt to answer. 
  - Make sure this is a clear, answerable question. Don't do "a study of... ". 
  - Make sure this is genuinely a question about a *time series process*. 
    - Imagine a metronome: each time you hear a 'click', another row of data values gets added to your table.


## Identify a time series data set you want to work with

Ideally, identify a data set that you that you might use as the basis for your class project. You need not necessarily commit at this time to using this data set for your project. However, bear in mind that you will in this assignment be investing time and effort in acquiring, cleaning, and organizing the data set. It is better for you if you invest that effort on the data set you will later analyze.

Please do not use a data set that already come packaged with `R` or that is otherwise already cleaned up. The point of this assignment is to learn how to use tools from the Tidyverse and tidyverts packages when working with data you acquire "in the wild".

## Acquire the data from its source location, reproducibly

You typically will need first to *extract* your data from its original source (e.g., an Excel file, an API, a database, a cloud hosting service). 

## Stage your raw data

If your data files are smaller than 25 MB, you may synch them to Github with the rest of your repo. Put them in the `raw_data` folder.

Files larger than 25 MB should not be posted to Github. Instead, post them to a cloud storage service such as Google Drive, One Drive, Box, or Dropbox. Your code should include a reproducible procedure for downloading the data in these files to a local drive.


## Submission procedure

Create a new R Markdown document for this work, inside R Studio. Code and document all these steps in a single .Rmd file. 

To submit your work: knit your .Rmd file to generate a .pdf file. Push both the .Rmd and .pdf files to your Repo on Github, along with any supporting files. Submit your assignment on Collab, enclosing a link to your .pdf file on Github. 

## Other comments

All steps should be fully *reproducible*. This means: If I, or anyone, rerun the code in your R Markdown file and reknit the .Rmd file to generate a new PDF output, I should be able to execute all your computational steps and regenerate your PDF essentially exactly. 



<!--chapter:end:19-assignment-get_data.Rmd-->

# Data preparation strategy

Topics:

* Tidy data
* `tsibble` objects for storing, manipulating, and visualizing time series data. Frequency of time series: the `index` parameter. `key` parameter(s).
* Applying `dplyr` verbs to `tsibble` objects: `filter`, `select`, `mutate`, `group_by`, `summarize`



Readings:

* [TSDS, Sections 3.1-3.9](https://jhudatascience.org/tidyversecourse/wrangle-data.html#anonymous-functions)
* FPP3, Section 2.1

Optional: To learn how to wrangle and visualize data using the Tidyverse packages, you may find it useful to go through the [Tidyverse Fundamentals with R](https://learn.datacamp.com/skill-tracks/tidyverse-fundamentals) modules on [Datacamp](https://learn.datacamp.com/). Datacamp also offers [a range of other learning modules for developing data science skills in R](https://learn.datacamp.com/career-tracks/data-scientist-with-r). 

Assignment: [Extract and prepare your data](https://github.com/uva-eng-time-series-sp21/sys5581-course-materials/blob/master/assignments/assignment_data_etl.pdf).

## Overview: Extraction, transformation, and loading of data

Before undertaking any data analysis project, you need to organize your data into a format to make it ready for analysis. Very commonly, the data you wish to work with will not come to you in a nice format that makes it ready to analyze. Often it will be necessary to transform the data, applying a sequence of manipulations to get it into a nice format such as a single table. 

If you have saved your prepared table to a database or local file, your may finally need to load the data into memory on your working machine as a prelude to commence analysis. These steps together are the *extract-transform-load* (ETL) stage of a data analysis project.

The bad news is that working data scientists generally report that the ETL stage is the most time-consuming part of a data science project. The good news is that the `R` `tidyverse` packages offer a number of helpful tools to somewhat ease the pain of ETL work, also known informally as *data wrangling*.[^1]

[^1]: "Wrangling" refers to work with cattle, sheep, and other livestock.



The ETL steps needed for a given project will depend on the nature of the data and on how they are originally organized. We can characterize how we want the data to look at the end of the ETL stage. To the extent possible, we want the data to be *tidy*.


## Organize your data into a *tidy* data frame

Readings: [TSDS Sections 1.2-1.3](https://jhudatascience.org/tidyversecourse/intro.html#tidy-data)


Hadley Wickham [-@wickhamTidyData2014] codifies the concept of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html):

> Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:

> 1. Each variable forms a column.
> 2. Each observation forms a row.
> 3. Each type of observational unit forms a table.

"Real datasets can, and often do, violate the three precepts of tidy data in almost every way imaginable. While occasionally you do get a dataset that you can start analysing immediately, this is the exception, not the rule." In real life, the systems people and organizations use to collect, manage, and store data are governed by many priorities: end-user convenience, clarity of presentation, storage costs, processing speed, etc. Making data fit for analysis by data scientists is typically a minor consideration, if it is thought of at all.

Expanding on the theme, Wickham identifies five of the most common problems with non-tidy, "messy" datasets: 

> * Column headers are values, not variable names.
> * Multiple variables are stored in one column.
> * Variables are stored in both rows and columns.
> * Multiple types of observational units are stored in the same table.
> * A single observational unit is stored in multiple tables.

The Tidyverse packages integrate a range of tools to help with transforming the messy data often encountered in the wild into tidy formats more suitable for analysis. See the [Tidyverse website](https://www.tidyverse.org/) for an overview, or [this Coursera course](https://www.coursera.org/specializations/tidyverse-data-science-r) for more guidance.

* Organize your data table(s) so that they 'match' your question.

## Convert your data into a `tsibble` object

In this step of the assignment, you will convert your tidy data frame into a `tsibble` object. Doing so in effect tells `R`: "These data actually form a time series. One column, which I designate the `index`, contains time values, in equal intervals." Taking this step enables the use of specific tools for data visualization, exploratory analysis, and forecasting for time series data. The [tsibble package](https://tsibble.tidyverts.org/) for R provides "a data infrastructure for tidy temporal data". Review the documentation for instructions. 


## Data preparation strategy: Design your end-point data table(s) 

Starting point: Multiple source files, mess, etc. This is real life as a data scientist.

What's your desired end point? How will you prepare your data to make it ready to analyze?

Data preparation is a creative activity. (Your jobs are secure.)

### Design your end point first (at least, in your head).

* Which columns? In which order?
* Which data types should the different columns have?
* For `tsibble` objects: 
  - Which column is the `index`? Must have date+time values, at regular intervals.
  - Which columns contain `key` values? 
      - These are values that *don't* change with time.
      - Each value of the `key` corresponds to a distinct time series.
      - Cannot have duplicate rows that share an `index` + `key` value.
  - Remaining columns contain observational data: one row for each time step and `key` value.
      - What data types should these be?
      - Might choose to drop columns you aren't going to use, to reduce clutter.


### Typical structure for a time series data table {#ts-table}

 | Date + time  |  Series |  Value_1 | Value_2 | Value_3 |
 |:-----------:|:----------:|:-------:|:-------:|:--------:|
 | 2020-02-01  |   "Virginia" | 33.57  | 29     |  "friendly" |
 | 2020-02-01  |   "Idaho" | 0.22  | 18     |  "hostile" |
 |   ...          |  ...   | ...    | ... | ... |       
 |  `index`     |   `key`  |         |          |          |
 |  [date]      |   [fctr] |  [dbl]  |  [int]   |  [fctr]  | 


Then wrangle your data to get to your desired end point.

Recommended practices:

* Put `index` field in the left-most column.
* Next, put all the `key` fields.
* Then finally the data values. Start with the most important ones.


## Additional resources and next steps

To learn how to wrangle and visualize data using the Tidyverse packages, you may find it useful to go through the [Tidyverse Fundamentals with R](https://learn.datacamp.com/skill-tracks/tidyverse-fundamentals) modules on [Datacamp](https://learn.datacamp.com/), or [the analogous course on Coursera](https://www.coursera.org/specializations/tidyverse-data-science-r). Datacamp also offers [a range of other learning modules for developing data science skills in R](https://learn.datacamp.com/career-tracks/data-scientist-with-r). 

With your data organized into a `tsibble` object, you are positioned to do some *exploratory data analysis*, the topic of the next assignment. If you want to get a head start, check out the functions for data visualization and exploratory analysis of time series in the [feasts package](https://feasts.tidyverts.org/). Or work through the examples in [Chapter 2](https://otexts.com/fpp3/graphics.html) or [Chapter 4](https://otexts.com/fpp3/features.html) of [Forecasting: Principles and Practice, 3rd ed.](https://otexts.com/fpp3/) 

<!--chapter:end:22-data-prep-strategy.Rmd-->

# Reading in data from source files


## First, examine text files in a text editor 

It's generally a good idea to examine the contents of an unfamiliar text data file visually, in text editor, before starting to work on it. You can open smaller files inside RStudio. For files too big to open inside RStudio, you can use a dedicated text editor like [Atom](https://atom.io/).

This code prints out the first ten lines of the text file "PetroStocks.csv".

```{r}
# Print out first ten rows of file "data/PetroStocks.csv"
readLines("data/PetroStocks.csv", n=10) %>% paste0(collapse="\n") %>% cat
```

What do you see?

## Read in CSV files

Using `read_csv()`: Example:

```{r Read in data from listed source}
# Data Source: https://www.eia.gov/petroleum/supply/weekly/
library(readr)

# Original code:
PetroStocksData <- readr::read_csv("data/PetroStocks.csv")
names(PetroStocksData) <- PetroStocksData[2,] # assign names to columns
PetroStocksData <- PetroStocksData[-1,]       # remove first unused row
PetroStocksData <- PetroStocksData[-1,]       # remove another unused row
```

### Declaring data types

Declaring data types as you read in data.

Choose your data types!

Reference: R4DS Ch. 15, 16

```{r Read in data from listed source - a more elegant solution}
### Revised using readr::read_csv() to skip empty rows and declare data types
### Declare column types within the read_csv() function call, to save yourself trouble later:
ps_tbl <- readr::read_csv("data/PetroStocks.csv", skip = 2, col_types = cols(.default = col_double(),
                                                                       "Date"   = col_character())) 

### Reference: https://readr.tidyverse.org/reference/read_delim.html
```








<!--chapter:end:23-data-read.Rmd-->

# Data cleaning

## Dropping empty rows and columns

```{r Drop empty columns - original vs revised, eval=FALSE}
# Original: empty column identified manually
PetroStocksData <- PetroStocksData[,-21] # Delete unused last column

### Revised: This code removes *all* empty columns:
ps_tbl %>% select_if(function(x) !all(is.na(x))) -> ps_tbl
```


```{r Drop empty rows - original vs revised, eval=FALSE}
# Original: empty row identified manually:
PetroStocksData <- PetroStocksData[-2004,] # Delete unused last row

### Revised: This code removes all empty rows:
ps_tbl %>% filter(!across(everything(), is.na)) -> ps_tbl
```



<!--chapter:end:24-data-clean.Rmd-->

# Transform a data frame to `tsibble` object

Readings: FPP3, Section 2.1

Convert the data frame into a time series `tsibble` object.

```{r}
# install.packages("tsibble")
library(tsibble) # Reference: https://tsibble.tidyverts.org/articles/intro-tsibble.html

esales <- arrow::read_feather("data/esales.feather")
esales %>%
  dplyr::select(date, sales_GWh = value) -> esales_tbl
esales_tbl %>% as_tsibble(index = date) -> elsales_tbl_ts

print(elsales_tbl_ts)
```


## Time indexing

See R4DS ch. 16.

Depending on how dates and times are recorded in your raw data, you may face more or less work to organize them into form(s) suitable as `tsibble` index variable.


The `lubridate` and `hms` packages may be valuable.

```{r Change the tsibble to index monthly}
# install.packages("feasts"), Reference: https://feasts.tidyverts.org/
library(feasts)

elsales_tbl_ts %>%
  mutate(Month = tsibble::yearmonth(date)) %>%
  as_tsibble(index = Month) %>%
  dplyr::select(Month,sales_GWh) -> vaelsales_tbl_ts

print(vaelsales_tbl_ts)
```


## Running diagnostics on your tsibble

** Ideally, should have exactly one row (i.e., one vector of measured values) for each time interval (`index`) and each value of the `key` variables. 
 -- May not have any duplicates.
 -- May have missing values
 
### Duplicate values 


### Missing values


### Irregular time intervals

<!--chapter:end:25-data-transform.Rmd-->

# Saving and loading data files

Your real analytic work can begin when you have a prepared, cleaned data file ready to load into memory. You probably don't want to re-do all the data preparation steps each time you start work.

If you are working with a small data file, then it is probably not a problem to re-run the code to prepare your data file: it only takes a second. But especially if you are working with a larger file, re-doing the data prep for each work session is a hassle.

Instead, you may wish to separate your data preparation code into its own script, then save the resulting prepared file to disk. Then, when you sit down to do analytic work, you can load your prepared file directly into memory.

## Fast file reading and writing: The `arrow` package

For saving and reading larger files, I recommend using the `feather` format supported by the [`arrow`](https://arrow.apache.org/docs/r/articles/arrow.html) package. Arrow's functions [`read_feather()`](https://arrow.apache.org/docs/r/reference/read_feather.html) and [`write_feather()`](https://arrow.apache.org/docs/r/reference/write_feather.html) work much faster than the corresponding read-write functions in base R. Arrow provides good support for work with large files, and [plays well with dplyr](https://arrow.apache.org/docs/r/articles/dataset.html). 

<!--chapter:end:26-data-save.Rmd-->

# Assignment: Prepare your data


This assignment focuses on the steps needed to clean, organize, and prepare your data set for time series analysis. You are asked to:

1. Read the data into R
1. Organize the data into a *tidy* data frame.
1. Clean the data and perform various data quality checks
2. Convert this data frame into a `tsibble` object to render it ready for time series analysis.
3. Generate at least one table or graph based on the data (more if you like).



## Generate at least one table or figure

This step simply confirms that you have completed the preparation process and that your data is ready to analyze. You could for example simply add a line of code: `print([name_of_your_tsibble])` to generate a simple table.







<!--chapter:end:29-assignment_data_prep.Rmd-->

# (PART) Exploratory Data Analysis {-} 

# Exploratory analysis of time series data

```{r set up coding environment, include=FALSE, message=FALSE}
# library(dplyr) -- don't need this if you are loading the entire 'tidyverse' suite
library(tidyverse)
library(lubridate) # For easy handling of time-indexed objects

# if(!('fpp3' %in% installed.packages())) install.packages('fpp3')
library(fpp3)
```

## Overview

Readings: 

* FPP3, Sections 2.2--2.6. 
* [TSDS, Section 3.10](https://jhudatascience.org/tidyversecourse/wrangle-data.html#exploratory-data-analysis), [Chapter 4](https://jhudatascience.org/tidyversecourse/dataviz.html) and [Section 5.5](https://jhudatascience.org/tidyversecourse/model.html#descriptive-and-exploratory-analysis)

Topics:

* Time series plots.
* Trends. Seasonal (periodic) patterns. Cycles.
* Seasonal plots. Seasonal sub-series.
* Investigating relationships between two variables. Scatterplots. Correlation. Scatterplot matrices.

Assignment: Explore your data.


## Briefly characterize the dataset

Provide a brief example of the data, showing how they are structured.

Example: Monthly electricity sales for Virginia

Previously we extracted monthly electricity sales data for Virginia from a remote database, converted the data frame into a `tibble` object, and saved the result to a file in feather format.


```{r read in data}
library(arrow)
esales <- read_feather("data/esales.feather")
```

```{r make basic data summaries}
print(esales)    # print the data as a table
summary(esales)  # compute basic summary statistics about the data
boxplot(esales)
hist(elsales_tbl_ts$sales_GWh, breaks=40) #  Make a histogram of monthly sales
```


### Examine subsets of the data

```{r use tidyverse syntax to perform some simple data manipulations, eval=FALSE}
# References: https://www.tidyverse.org/, https://dplyr.tidyverse.org/
# filter(data object, condition) : syntax for filter() command

esales %>%
  filter(year == 2019) %>%
  filter(value > 9000) %>%
  print()

(esales %>%
  group_by(year) %>%
  summarise(Total = sum(value)) -> total_esales_by_year)

esales %>%
  mutate(sales_TWh = value/1000) %>%
  dplyr::select(-value)
```


```{r}
# library(lubridate) # Make it easy to deal with dates

esales %>% filter(month==3)                   # These three lines of code
esales %>% filter(month(date)==3)             #   all do
esales %>% filter(lubridate::month(date)==3)  #   the same thing.

# We don't have to keep the 'year' and 'month' column: can recover them if needed

esales %>%
  dplyr::select(date, sales_GWh = value) -> esales_tbl

print(esales_tbl)
```


## Plot the time series

Ref: FPP3, Section 2.2


```{r use ggplot2 to generate a plot}
#Reference: https://ggplot2.tidyverse.org/

ggplot(data=esales, aes(x=date,y=value)) +
  geom_line() + xlab("Year") + ylab("Virginia monthly total electricity sales (GWh)")
```

```{r use feasts autoplot}
# feasts::autoplot() is handy for quickly generating time series plots

autoplot(vaelsales_tbl_ts, sales_GWh) +
  ylab("Virginia monthly total electricity sales (GWh)") +
  xlab("")  # Leave horiz. axis label blank
```


## Sesaonal plots

Ref: FPP3, Sections 2.3, 2.4

### Example: Virginia monthly electricity

Recall how we readied these data:

```{r}
esales <- arrow::read_feather("data/esales.feather")
esales %>%
  dplyr::select(date, sales_GWh = value) -> esales_tbl

esales_tbl %>% as_tsibble(index = date) -> elsales_tbl_ts

print(elsales_tbl_ts)
```


```{r, eval=FALSE}
# This plot won't work. Why not?
elsales_tbl_ts %>%
  feasts::gg_season(sales_GWh, labels = "both") + ylab("Virginia electricity sales (GWh)")
```

```{r Change the tsibble so index is monthly}
# install.packages("feasts"), Reference: https://feasts.tidyverts.org/
library(feasts)

elsales_tbl_ts %>%
  mutate(Month = tsibble::yearmonth(date)) %>%
  as_tsibble(index = Month) %>%
  dplyr::select(Month,sales_GWh) -> vaelsales_tbl_ts

print(vaelsales_tbl_ts)
```


```{r , warning=FALSE}
vaelsales_tbl_ts %>% gg_season(sales_GWh, labels = "both") + ylab("Virginia electricity sales (GWh)")
```

### Example: Australian production

```{r, warning=FALSE}
# install.packages('tsibbledata')
library(tsibbledata)

aus_production

aus_production %>% gg_season(Electricity)

aus_production %>% gg_season(Beer)
```


### Seasonal subseries plots

Ref: FPP3, Section 2.5

```{r}
vaelsales_tbl_ts %>%
  gg_subseries(sales_GWh) +
  labs(
    y = "Sales (GWh)",
    title = "Seasonal subseries plot: Virginia electricity sales"
  )
```


## Scatterplots

Readings: FPP Sect. 2.6

Investigating relationships between two variables. Scatterplots. Correlation. Scatterplot matrices.

```{r}
vic_elec

summary(vic_elec)

vic_elec %>%
  filter(year(Time) == 2013) %>%
  autoplot(Demand) +
  labs(
    y = "Demand (GW)",
    title = "Half-hourly electricity demand: Victoria"
  )
```

```{r}
vic_elec %>%
  filter(year(Time) == 2013) %>%
  autoplot(Temperature) +
  labs(
    y = "Temperature (degrees Celsius)",
    title = "Half-hourly temperatures: Melbourne, Australia"
  )
```

```{r}
vic_elec %>%
  filter(year(Time) == 2013) %>%
  ggplot(aes(x = Temperature, y = Demand)) +
#  geom_density2d() +
  geom_point(size=0.1, aes(colour=Holiday), alpha = 0.4) +
  labs(y = "Demand (GW)", x = "Temperature (degrees Celsius)")
```

### A Scatterplot matrix

```{r, warning=FALSE}
vic_elec

boxplot(vic_elec$Temperature)

# install.packages("GGally")
vic_elec %>%
  # mutate(Temperature = round(Temperature)) %>%
  # pivot_wider(values_from=c(Demand,Temperature), names_from=Holiday) %>%
  GGally::ggpairs(columns = 3:2)

vic_elec %>%
  mutate(Year = factor(year(Date))) %>%
  dplyr::select(-c(Date, Holiday)) %>%
  GGally::ggpairs(columns = 4:2)
```



<!--chapter:end:31-explore-basics.Rmd-->

# Time series decomposition

Readings: FPP3, Sections 3.1, 3.2


### Example: Gross Domestic Product data


```{r}
library(tsibbledata) # Data sets package

print(global_economy)
```

```{r}
global_economy %>% filter(Country=="Sweden") %>% print()
```

```{r}
global_economy %>%
  filter(Country=="Sweden") %>%
  autoplot(GDP) +
  ggtitle("GDP for Sweden") + ylab("$US billions")
```



### Fitting data to simple models

```{r}
global_economy %>% model(trend_model = TSLM(GDP ~ trend())) -> fit

fit
```

```{r}
fit %>% filter(Country == "Sweden") %>% residuals()
```

```{r}

fit %>% filter(Country == "Sweden") %>% residuals() %>% autoplot(.resid)
```

### Work with ln(GDP)

```{r}
global_economy %>%
  filter(Country=="Sweden") %>%
  autoplot(log(GDP)) +
  ggtitle("ln(GDP) for Sweden") + ylab("$US billions")
```

```{r}
global_economy %>%
  model(trend_model = TSLM(log(GDP) ~ trend())) -> logfit
```

```{r}
logfit %>% filter(Country == "Sweden") %>% residuals() %>% autoplot()
```

```{r}
global_economy %>% model(trend_model = TSLM(log(GDP) ~ log(Population))) -> fit3

fit3 %>% filter(Country == "Sweden") %>% residuals() %>% autoplot()

```

## Producing forecasts

```{r}
fit %>% forecast(h = "3 years") -> fcast3yrs

fcast3yrs

```

```{r}

fcast3yrs %>% filter(Country == "Sweden", Year == 2020) %>% str()
```

```{r visualize forecasts}
fcast3yrs %>%
  filter(Country=="Sweden") %>%
  autoplot(global_economy) +
  ggtitle("GDP for Sweden") + ylab("$US billions")
```

### Model residuals vs. forecast errors

Model residuals:

Your data: $y_1, y_2, \ldots, y_T$

Fitted values: $\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_T$

Model residuals: $e_t = y_t - \hat{y}_t$

Forecast errors:

```{r}
augment(fit)
```

```{r}
augment(fit) %>% filter(Country == "Sweden") %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 20) +
  ggtitle("Histogram of residuals")
```

### Are the model residuals auto-correlated?

```{r}
augment(fit) %>% filter(Country == "Sweden") -> augSweden

augSweden %>%
  ACF(.resid) %>%
  autoplot() + ggtitle("ACF of residuals")
```

```{r}
augment(fit3) %>% filter(Country == "Sweden") -> augSweden3

augSweden3 %>%
  ACF(.resid) %>%
  autoplot() + ggtitle("ACF of residuals")
```


## Example: GDP, several countries


```{r}
library(tsibbledata) # Data sets package

nordic <- c("Sweden", "Denmark", "Norway", "Finland")

(global_economy %>% filter(Country %in% nordic) -> nordic_economy)

```

```{r}
nordic_economy %>% autoplot(GDP)
```

```{r}
fitnord <- nordic_economy %>%
  model(
    trend_model = TSLM(GDP ~ trend()),
    trend_model_ln = TSLM(log(GDP) ~ trend()),
    ets = ETS(GDP ~ trend("A")),
    arima = ARIMA(GDP)
  )

fitnord
```

```{r}
fitnord %>%
  dplyr::select(arima) %>%
  coef()
```


Denmark: ARMA(1,1)

Finland: MA(2)

Norway: MA(1)

Sweden: MA(2)

```{r}
nordic_economy %>%
  model(arima_constrained = ARIMA(GDP ~ pdq(1,0,2))) %>% dplyr::select(arima_constrained) %>% coef()
```

```{r}
fitnord %>% coef()
```

```{r}
fitnord %>%  glance()  
```

```{r}
fitnord %>% filter(Country == "Denmark") %>% dplyr::select(arima) %>% report()
```

```{r}
fitnord %>%
  accuracy() %>%
  arrange(Country, MPE)
```



```{r, eval=FALSE}
# ETS forecasts
USAccDeaths %>%
  ets() %>%
  forecast() %>%
  autoplot()
```

```{r, eval=FALSE}
str(taylor)
plot(taylor)
```

### Plot lagged values

```{r plot lagged values}
vaelsales_tbl_ts  %>% filter(month(Month) %in% c(3,6,9,12)) %>% gg_lag(sales_GWh, lags = 1:2)

vaelsales_tbl_ts  %>% filter(month(Month) == 1) %>% gg_lag(sales_GWh, lags = 1:2)
```

```{r}
vaelsales_tbl_ts %>% ACF(sales_GWh) %>% autoplot()
```

```{r perform automated time series decomposition}


# decompose(vaelsales_tbl_ts)
```

```{r perform additive STL decomposition of the VA electricity sales time series}
vaelsales_tbl_ts %>%
  model(STL(sales_GWh ~ trend(window=21) + season(window='periodic'), robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r perform multiplicative STL decomposition of the VA electricity sales time series}
vaelsales_tbl_ts %>%
  mutate(ln_sales_GWh = log(sales_GWh)) %>%
  model(STL(ln_sales_GWh ~ trend(window=21) + season(window='periodic'),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r}
vaelsales_tbl_ts %>%
  features(sales_GWh, feat_stl)
```

```{r}
vaelsales_tbl_ts %>%
  features(sales_GWh, feature_set(pkgs="feasts"))
```

<!--chapter:end:32-explore-ts-decomposition.Rmd-->

# Autocorrelation

Readings: FPP3 Sections 2.7-2.9, 4.2, 4.5




```{r plot lagged values 2}
vaelsales_tbl_ts  %>% filter(month(Month) %in% c(3,6,9,12)) %>% gg_lag(sales_GWh, lags = 1:2)

vaelsales_tbl_ts  %>% filter(month(Month) == 1) %>% gg_lag(sales_GWh, lags = 1:2)
```

```{r}
vaelsales_tbl_ts %>% ACF(sales_GWh) %>% autoplot()
```

## Heere be monsters

```{r}
knitr::include_graphics("graphics/horst_acf/horst_acf_1.jpg")
```


<!--chapter:end:34-explore-autocorrelation.Rmd-->

# (PART) Model Specification and Estimation {-} 

# The data-generating process

In Section \@ref(ts-table) we introduced a structure for a typical table of time series data:

 | Date        |  Series    |  Value_1 | Value_2 | Value_3  |
 |:-----------:|:----------:|:--------:|:-------:|:--------:|
 | 2020-02-01  |   "Virginia" | 33.57  | 29     |  "friendly" |
 | 2020-02-01  |   "Idaho" | 0.22  | 18     |  "hostile" |
 |   ...          |  ...   | ...    | ... | ... |       
 |  `index`     |   `key`  |         |          |          |
 |  [date]      |   [fctr] |  [dbl]  |  [int]   |  [fctr]  | 


Here, the `Date` field contains values for regular time intervals on which data are recorded. The fields `Value_1`, `Value_2`, etc., contain the actual observed values. The `Series` field reports objects (here, U.S. states) for which data are reported.

Let's make this setup both simpler and somewhat more general. For now, let's drop the key field `Series`, and suppose we are dealing only with one time series. Let's abstract from the `Date` field, and just label the sequence of time steps $t = 1,2,\ldots,T$. Let's suppose further we have only one sequence of data values $y_1, y_2, \ldots, y_T$. Our abstracted data table now looks like:

 | $t$         |  $y$    |  
 |:-----------:|:----------:|
 | $1$         |   $y_1$ | 
 | $2$         |   $y_2$ | 
 |   ...       |  ...   |        
 |  $T$        |   $y_T$  |  

Suppose we have a dataset structured in this way:
```{r simulate white noise, message=FALSE, echo=FALSE}
set.seed(1)       # Keeps random data from changing each time the code is run

theta <-  18      # True long-run average temp
sigma  <-  3      # True stnd dev of temp around this mean
n      <- 40      # n = number of simulated historical data points
                  #   We don't use 'T' as a variable name because in R, 'T' = logical 'TRUE'

y <- rnorm(n,theta,sigma)

library(dplyr)

y_tbl <- as_tibble(y) %>% 
  mutate(t = row_number()) %>%
  select(t, y = value) 
          
print(y_tbl)
```


Given such a dataset, the challenge we confront is how best to *explain* the process by which the observed data were generated, and also to *predict* future values from this process that haven't been observed yet.

To meet this challenge, the central step is to create a formal mathematical model of the data-generating process. To explain what that means, we'll start with a simple example.

## The white noise process

One of the simplest models of a process for generating these data is to treat them as *white noise*. We actually introduced this model previously, in Section \@ref(white-noise-model). 

In a white noise process, the data are generated as independent, identically distributed random draws from a fixed normal distribution. Formally, for $t = 1, \ldots, T$,

$$
y_t = \theta + \varepsilon_t
$$
 
where $\theta$ denotes the mean value of the process, and where $\varepsilon_t \sim N(0, \sigma^2)$ is a random noise term. Here, the parameters $\theta$ and $\sigma^2$ are assumed to be constant. The values of these parameters are not directly observable. Instead, their values must be estimated based on the observed data, using established techniques of statistical estimation. Since the data we have are limited and noisy, these estimates will in general be only approximately correct. The issue for practical work is whether these approximations are close enough to be useful in our specific application.

Before we get into statistical estimation techniques, let's take up a prior question: Why, or when, should we believe this model? What would make us think this model is a true --- or at least, serviceably close-enough --- representation of the underlying process that generated the observed data? 

A reasonable approach is to evaluate whether the data "look like" what we would expect to see, if the model were true. If the white noise model were the true model of the data-generating process, then several observable features in the data should be apparent.

### Stationarity

First, the process would be *stationary*. Formally, a stochastic process is *stationary* if its unconditional joint probability distribution does not change when shifted in time. There should be no discernible upward or downward trend over time, for example.

There are (of course) statistical tests for stationarity one can apply to a time series to estimate the likelihood that the series (more exactly: the underlying stochastic process that generated the series) is stationary. But before applying such tests, it's a good idea to just plot the data and ask: do they *look* stationary?

```{r}
library(ggplot2)
library(ggthemes)

p <- ggplot(y_tbl, aes(t, y)) + geom_point() + xlab(expression(t)) + ylab(expression(y[t])) + theme_clean()
p
```

Just by visual inspection, there doesn't *seem* to be much of a trend. 

Fitting a linear model to the data and plotting the regression line reveals a very shallow downward trend:

```{r}
p + geom_smooth(method = "lm", se = FALSE)
```

Hmmm. Is this trend "real"? Or did it just happen that, by random chance, the data later in the series happen to have slightly lower values on average than those generated earlier? 

This question is very typical of those that arise in the challenge of *model selection*, i.e., of choosing the model that best explains the data generating process. It is sometimes difficult to tell whether a pattern we observe in our data corresponds to a real feature of the underlying process, or if the feature is instead a kind of illusion --- one that just happened to emerge from the particular data we observe, due to random chance, but that would not necessarily be observed from another sample of data drawn from the same process.

We could (with caution) test the statistical significance of the trend term:
```{r}
y_tbl %>% lm(y ~ t, data = .) -> temp.lm

print(temp.lm)
coef(temp.lm)
```



### No autocorrelation


  * Temperatures are assumed to be *independent* from one year to the next. In particular, there is no *autocorrelation*. Knowing that one year's temperature was unusually high (say) provides no information about the likelihood that next year's temperature will also be unusually high. Inter-annual climate cycles (e.g., due to $El\ Ni\tilde{n}o$) are ruled out.
  
  * Temperature variations around the long-run average are assumed to be *identically distributed*. This assumption rules out the possibility that variance is, say, greater when temperatures are higher than when they are lower. Nor do they grow over time.
  




<!-- For the electricity sales data, maybe the process looks like: -->

<!-- $$ y_t = Trend_t X Seasonal_t X Residual_t $$ -->

<!-- $$ y_t = \beta_0 + \beta_1 t + \beta_2 m + \varepsilon_t $$ -->

<!--chapter:end:41-model-specification.Rmd-->

# The normal linear model

$$ y_t = \beta_0 + \beta_1 x_t + \varepsilon_t $$

## Assumptions of the linear model

* Relationship between predictor $x$ and predictand $y$ is linear.
* Both $x$ and $y$ are known, observed without error.

* Errors have mean zero.
* Errors are independent of each other.  
* Errors are uncorrelated with predictor variables $x_t$.

Often, assume stronger additional conditions that errors are *independent, identically normally distributed*: for all $t$, $\varepsilon_t \sim N(0, \sigma^2)$. for a constant $\sigma^2$.

In compact vector and matrix notation, we may write: 

$$ 
Y = X \beta + \varepsilon,
\quad \text{where $\varepsilon \sim N(0, \sigma^2 I_T)$} 
$$ 

Readings: FPP, Section 7.1

## Examples of the normal linear model





<!--chapter:end:42-model-normal-linear.Rmd-->

# Ordinary least squares estimation

Regression coefficients $\beta$ and error variance $\sigma^2$ are unobserved: their values must be estimated from the data. Various estimation techniques may be used.

<!--chapter:end:43-model-ols.Rmd-->

# Assignment: Project proposal

In this assignment you will develop your initial concept note into a draft of a full project proposal. Treat this assignment as a "dry run" for developing a proposal for a grant or fellowship application, or for your Ph.D. prospectus.

Your proposal should include at least the following sections and information.

**Front matter:** Descriptive title, your name, date, reference to "SYS 5581 Time Series & Forecasting, Spring 2021".

**Abstract:** A very brief summary of the project.

## Introduction

Give a narrative description of the problem you are addressing, and the methods you will use to address it. Provide context:

-   What is the question you are attempting to answer?
-   Why is this question important? (Who cares?)
-   How will you go about attempting to answer this question?

This work addresses the question: Why do people not use probabilistic forecasts for decision-making?

## The data and the data-generating process

Describe the data set you will be analyzing, and where it comes from, how it was generated and collected. Identify the source of the data. Give a narrative description of the data-generating process: this piece is critical.

Since these will be time series data: identify the frequency of the data series (e.g., hourly, monthly), and the period of record.


## Exploratory data analysis


Provide a brief example of the data, showing how they are structured.


## Plot the time series.


## Perform and report the results of other exploratory data analysis

### STL decomposition

```{r perform additive STL decomposition of the VA electricity sales time series 2}
vaelsales_tbl_ts %>%
  model(STL(sales_GWh ~ trend(window=21) + season(window='periodic'), robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r perform multiplicative STL decomposition of the VA electricity sales time series 2}
vaelsales_tbl_ts %>%
  mutate(ln_sales_GWh = log(sales_GWh)) %>%
  model(STL(ln_sales_GWh ~ trend(window=21) + season(window='periodic'),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r}
vaelsales_tbl_ts %>% features(sales_GWh, feat_stl)
```

```{r}
vaelsales_tbl_ts %>% features(sales_GWh, feature_set(pkgs="feasts"))
```



### Fitting data to simple models

```{r}
global_economy %>% model(trend_model = TSLM(GDP ~ trend())) -> fit

fit


```

```{r}
fit %>% filter(Country == "Sweden") %>% residuals()
```

```{r}

fit %>% filter(Country == "Sweden") %>% residuals() %>% autoplot(.resid)
```

### Work with ln(GDP)

```{r}
global_economy %>%
  filter(Country=="Sweden") %>%
  autoplot(log(GDP)) +
  ggtitle("ln(GDP) for Sweden") + ylab("$US billions")
```

```{r}
global_economy %>%
  model(trend_model = TSLM(log(GDP) ~ trend())) -> logfit
```

```{r}
logfit %>% filter(Country == "Sweden") %>% residuals() %>% autoplot()
```

```{r}
global_economy %>% model(trend_model = TSLM(log(GDP) ~ log(Population))) -> fit3

fit3 %>% filter(Country == "Sweden") %>% residuals() %>% autoplot()

```



## Statistical model

### Formal model of data-generating process

Write down an equation (or set of equations) that represent the data-generating process formally.


If applicable: describe any transformations of the data (e.g., differencing, taking logs) you need to make to get the data into a form (e.g., linear) ready for numerical analysis.

What kind of process is it? $AR(p)$? White noise with drift? Something else?

Write down an equation expressing each realization of the stochastic process $y_t$ as a function of other observed data (which could include lagged values of $y$), unobserved parameters ($\beta$), and an error term ($\varepsilon_t$). Ex:

$$y = X\cdot\beta + \varepsilon$$ Add a model of the error process. Ex: $\varepsilon \sim N(0, \sigma^2 I_T)$.

### Discussion of the statistical model

Describe how the formal statistical model captures and aligns with the narrative of the data-generating process. Flag any statistical challenges raised by the data generating process, e.g. selection bias; survivorship bias; omitted variables bias, etc.

## Plan for data analysis

Describe what information you wish to extract from the data. Do you wish to... estimate the values of the unobserved model parameters? create a tool for forecasting? estimate the exceedance probabilities for future realizations of $y_t$?

Describe your plan for getting this information. OLS regression? Some other statistical technique?

If you can: describe briefly which computational tools you will use (e.g., R), and which packages you expect to draw on.

## Submission requirements

Prepare your proposal using Markdown . (You may find it useful to generate your Markdown file from some other tool, e.g. R Markdown in R Studio.) Submit your proposal by pushing it to your repo within the course organization on Github. When your proposal is ready, notify the instructor by also creating a submission for this assignment on Collab. Please also upload a PDF version of your proposal to Collab as part of your submission.

## Comment

Depending on your prior experience, you may find this assignment challenging. Treat this assignment as an opportunity to make progress on your own research program. Make your proposal as complete as you can. But note that this assignment is merely the First Draft. You will have more opportunity to refine your work over the next two months, in consultation with the instructor, your advisor, and your classmates.

## References

<!--chapter:end:49-assignment-project_proposal.Rmd-->

# (PART) Model evaluation {-}

# Evaluating model residuals


## Model residuals vs. forecast errors

Model residuals:

Your data: $y_1, y_2, \ldots, y_T$

Fitted values: $\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_T$

Model residuals: $e_t = y_t - \hat{y}_t$

Forecast errors:

```{r}
augment(fit)
```

```{r}
augment(fit) %>% filter(Country == "Sweden") %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 20) +
  ggtitle("Histogram of residuals")
```

## Are the model residuals auto-correlated?

```{r}
augment(fit) %>% filter(Country == "Sweden") -> augSweden

augSweden %>%
  ACF(.resid) %>%
  autoplot() + ggtitle("ACF of residuals")
```

```{r}
augment(fit3) %>% filter(Country == "Sweden") -> augSweden3

augSweden3 %>%
  ACF(.resid) %>%
  autoplot() + ggtitle("ACF of residuals")
```


## Example: GDP, several models, several countries


```{r}
library(tsibbledata) # Data sets package

nordic <- c("Sweden", "Denmark", "Norway", "Finland")

(global_economy %>% filter(Country %in% nordic) -> nordic_economy)

```

```{r}
nordic_economy %>% autoplot(GDP)
```

```{r}
fitnord <- nordic_economy %>%
  model(
    trend_model = TSLM(GDP ~ trend()),
    trend_model_ln = TSLM(log(GDP) ~ trend()),
    ets = ETS(GDP ~ trend("A")),
    arima = ARIMA(GDP)
  )

fitnord
```



Denmark: ARMA(1,1)

Finland: MA(2)

Norway: MA(1)

Sweden: MA(2)


```{r}
fitnord %>% coef() 
```

```{r}
fitnord %>%  glance()  
```


```{r}
fitnord %>%
  accuracy() %>%
  arrange(Country, MPE)
```



<!--chapter:end:50-model-evaluation.Rmd-->

# (PART) Forecasting {-}


# Producing forecasts

```{r}
fit %>% forecast(h = "3 years") -> fcast3yrs

fcast3yrs

```

```{r}

fcast3yrs %>% filter(Country == "Sweden", Year == 2020) %>% str()
```

```{r visualize forecasts 2}
fcast3yrs %>% 
  filter(Country=="Sweden") %>%
  autoplot(global_economy) +
  ggtitle("GDP for Sweden") + ylab("$US billions")
```

<!--chapter:end:60-forecasting.Rmd-->

# Communicating forecast uncertainty

```{r, echo=FALSE}
knitr::include_graphics(here::here("graphics","forecast_communications","nas-completing_the_forecast-cover_image.jpg"))
```

## The Red River Flood of 1997



```{r, echo=FALSE, caption }
knitr::include_graphics(here::here("graphics","forecast_communications","Headline_from_The_Forum_newspaper_April_24_1997.png"))
```

```{r, echo=FALSE}
knitr::include_graphics(here::here("graphics","forecast_communications","Deterministic_forecasts_issued_by_NWS_prior_to_the_Red_River_flood_of_1997.png"))
```

```{r, echo=FALSE}
knitr::include_graphics(here::here("graphics","forecast_communications","Probabilistic_river_stage_forecast_from_AHPS-SOURCE_NWS.png"))
```

<!--chapter:end:62-forecasting_uncertainty.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:90-references.Rmd-->

